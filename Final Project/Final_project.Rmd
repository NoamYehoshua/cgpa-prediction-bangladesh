---
title: "Final Project Notebook"
author: "Team 2"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: sentence
---

# Import libraries:

```{r ,include=FALSE}
library(knitr)
library(tidyverse)
library(broom)
library(htmltools)
library(readxl)
library(tidymodels)
library(rsample)
library(dplyr)
library(yardstick)
library(ranger)
library(ggplot2)
library(reshape2)
library(tibble)
library(gridExtra)
library(janitor)
library(corrplot)
```

# Global setup chunk

```{r setup, include = FALSE}
opts_chunk$set(echo=FALSE) 
```

# Read Data

```{r, include=FALSE}
data_raw <- read_excel("Students_Performance_data_set.xlsx")
```

# clean and organize the data:

```{r}
# -------------------- Rename Columns --------------------
# Give short, intuitive names to all variables
data <- data_raw %>%
  transmute(
    cgpa = `What is your current CGPA?`,
    sgpa = `What was your previous SGPA?`,
    study_hours = `How many hour do you study daily?`,
    study_freq = `How many times do you seat for study in a day?`,
    attendance = `Average attendance on class`,
    social_media = `How many hour do you spent daily in social media?`,
    skill_hours = `How many hour do you spent daily on your skill development?`,
    age = Age,
    gender = Gender,
    income = `What is your monthly family income?`,
    learning_mode = `What is your preferable learning mode?`,
    teacher_consult = `Do you attend in teacher consultancy for any kind of academical problems?`,
    smartphone = `Do you use smart phone?`,
    pc = `Do you have personal Computer?`,
    transport = `Do you use University transportation?`,
    english = `Status of your English language proficiency`,
    probation = `Did you ever fall in probation?`,
    suspension = `Did you ever got suspension?`,
    scholarship = `Do you have meritorious scholarship ?`,
    admission_year = `University Admission year`,
    hsc_year = `H.S.C passing year`,
    semester = `Current Semester`,
    credits = `How many Credit did you have completed?`,
    health_issues = `Do you have any health issues?`,
    disability = `Do you have any physical disabilities?`,
    relationship = `What is your relationship status?`,
    co_curricular = `Are you engaged with any co-curriculum activities?`,
    living_with = `With whom you are living with?`,
    interests = `What is you interested area?`,
    skills = `What are the skills do you have ?`
  )


# -------------------- Check for Missing Values --------------------
cat("Check for Missing Values:\n")
na_summary <- sapply(data, function(x) sum(is.na(x)))
na_summary <- sort(na_summary, decreasing = TRUE)
print(na_summary)


# -------------------- Convert ranges to numeric (e.g., "80-90" -> 85) --------------------
convert_range <- function(x) {
  if (is.na(x)) return(NA)
  if (grepl("-", x)) {
    bounds <- as.numeric(unlist(strsplit(x, "-")))
    return(mean(bounds, na.rm = TRUE))
  } else {
    return(as.numeric(x))
  }
}

# Apply to all columns with potential range strings
range_vars <- c("attendance", "income", "cgpa", "sgpa", "study_hours", "social_media", "skill_hours", "credits")

for (var in range_vars) {
  data[[var]] <- sapply(data[[var]], convert_range)
}



# -------------------- Convert columns to appropriate types --------------------
# Binary categorical variables to factor
binary_factors <- c("teacher_consult", "smartphone", "pc", "transport", "probation", 
                    "suspension", "scholarship", "health_issues", "disability", "co_curricular")

data[binary_factors] <- lapply(data[binary_factors], factor)

# Multi-category variables to factor
multi_factors <- c("gender", "learning_mode", "english", "relationship", "living_with", "interests", "skills")

data[multi_factors] <- lapply(data[multi_factors], factor)

# Numeric variables (already handled via convert_range)
numeric_vars <- c("cgpa", "sgpa", "study_hours", "study_freq", "attendance", "social_media", 
                  "skill_hours", "age", "income", "semester", "credits", "hsc_year")

data[numeric_vars] <- lapply(data[numeric_vars], as.numeric)


# -------------------- Detect Failed Numeric Conversions --------------------
failed_numeric <- list()

for (var in numeric_vars) {
  original <- data_raw[[names(data)[which(names(data) == var)]]]
  converted <- data[[var]]
  
  failed <- which(!is.na(original) & is.na(converted))
  
  if (length(failed) > 0) {
    failed_numeric[[var]] <- unique(original[failed])
  }
}

# Print problematic values that failed to convert to numeric
cat("\nFailed numeric conversions:\n")
print(failed_numeric)



# -------------------- Detect Unique Values After Factor Conversion --------------------
factor_vars <- c(binary_factors, multi_factors)
factor_issues <- list()

for (var in factor_vars) {
  factor_issues[[var]] <- unique(data[[var]])
}

cat("\nUnique values per factor column:\n")
print(factor_issues)

# -------------------- Fix Duplicate Values After Factor Conversion --------------------

# -------- Fix relationship-------------
data$relationship <- as.character(data$relationship)
data$relationship <- tolower(data$relationship)

# Normalize values
data$relationship <- case_when(
  str_detect(data$relationship, "single") ~ "Single",
  str_detect(data$relationship, "married") ~ "Married",
  str_detect(data$relationship, "engaged") ~ "Engaged",
  str_detect(data$relationship, "relationship") ~ "In a relationship",
  TRUE ~ NA_character_
)

data$relationship <- factor(data$relationship, levels = c("Single", "In a relationship", "Engaged", "Married"))


# -------- Fix health issues-------------
data$health_issues <- tolower(as.character(data$health_issues))

data$health_issues <- case_when(
  data$health_issues %in% c("no", "n") ~ "No",
  data$health_issues %in% c("yes", "y") ~ "Yes",
  TRUE ~ NA_character_
)

data$health_issues <- factor(data$health_issues, levels = c("No", "Yes"))

# -------- Fix interests-------------
# Normalize `interests` values
data$interests <- tolower(as.character(data$interests))
data$interests <- trimws(data$interests)

# Unify similar or misspelled categories (updated!)
data$interests <- case_when(
  str_detect(data$interests, "data schince") ~ "Data Science",
  str_detect(data$interests, "data science") ~ "Data Science",
  str_detect(data$interests, "machine learning") ~ "Machine Learning",  # now separate
  
  str_detect(data$interests, "cyber security|cybersecurity|syber security") ~ "Cyber Security",
  
  str_detect(data$interests, "web development|web developing|web developement|full stack") ~ "Web Development",

  str_detect(data$interests, "blockchain") ~ "Blockchain",
  str_detect(data$interests, "ai|artificial intelligence") ~ "AI",
  str_detect(data$interests, "programming|competitive") ~ "Programming",
  str_detect(data$interests, "ui/ux") ~ "UI/UX",
  str_detect(data$interests, "software") ~ "Software",
  str_detect(data$interests, "hardware") ~ "Hardware",
  str_detect(data$interests, "entrepreneur") ~ "Entrepreneurship",
  str_detect(data$interests, "networking") ~ "Networking",
  str_detect(data$interests, "teaching") ~ "Teaching",
  str_detect(data$interests, "event") ~ "Event Management",
  str_detect(data$interests, "creating a game") ~ "Game Development",
  str_detect(data$interests, "confuse") ~ NA_character_,
  TRUE ~ "Other"
)

data$interests <- factor(data$interests)

# -------- Fix skills-------------
# Clean and unify skill entries with updated mapping
data$skills <- tolower(as.character(data$skills))
data$skills <- trimws(data$skills)

data$skills <- case_when(
  str_detect(data$skills, "no skill|none|nothing|don't have any|not yet|not any|haven't started|i don't|do not know|basic|learner|trying to learn|memorizing|its on learning") ~ "No Skills",
  
  str_detect(data$skills, "networking, mis|networking") ~ "Networking",
  
  str_detect(data$skills, "programming|software development") ~ "Programming",
  str_detect(data$skills, "machine learning") ~ "Machine Learning",
  str_detect(data$skills, "artificial intelligence") ~ "AI",
  str_detect(data$skills, "cyber security|cybersecurity|syber security") ~ "Cyber Security",
  str_detect(data$skills, "web development|web development skill") ~ "Web Development",
  str_detect(data$skills, "data entry") ~ "Data Entry",
  str_detect(data$skills, "database") ~ "Database",
  str_detect(data$skills, "app development") ~ "App Development",
  str_detect(data$skills, "graphic|graphics designing") ~ "Graphic Design",
  str_detect(data$skills, "communication") ~ "Communication",
  str_detect(data$skills, "digital marketing") ~ "Digital Marketing",
  str_detect(data$skills, "video editing|videography") ~ "Video Editing",
  str_detect(data$skills, "python") ~ "Python",
  str_detect(data$skills, "system analysis") ~ "System Analysis",
  str_detect(data$skills, "content creation") ~ "Content Creation",
  str_detect(data$skills, "mentoring") ~ "Mentoring",
  str_detect(data$skills, "teaching") ~ "Teaching",
  str_detect(data$skills, "e-commerce") ~ "E-Commerce",
  str_detect(data$skills, "market analysis") ~ "Market Analysis",
  str_detect(data$skills, "problem solving") ~ "Problem Solving",
  str_detect(data$skills, "learning frontend") ~ "Frontend",
  str_detect(data$skills, "photographey|photography") ~ "Photography",
  TRUE ~ "Other"
)

data$skills <- factor(data$skills)

data$skills <- factor(data$skills)

# -------------------- Detect Unique Values After Factor Conversion --------------------
factor_vars <- c(binary_factors, multi_factors)
factor_issues <- list()

for (var in factor_vars) {
  factor_issues[[var]] <- unique(data[[var]])
}

#cat("\nUnique values per factor column after fix:\n")
#print(factor_issues)

# Table for interests
cat("\nFrequency of INTERESTS:\n")
print(sort(table(data$interests), decreasing = TRUE))

# Table for skills
cat("\nFrequency of SKILLS:\n")
print(sort(table(data$skills), decreasing = TRUE))



# -------------------- Create two more coulms for groups in "interest" and "skills" --------------------

# -------- Create unified interest groups -----------
data$interests_group <- case_when(
  data$interests %in% c("Web Development", "Programming", "Software", "Game Development") ~ "Software",
  data$interests == "Cyber Security" ~ "Cyber",
  data$interests %in% c("AI", "Machine Learning", "Data Science") ~ "Data/AI",
  data$interests %in% c("UI/UX") ~ "Design",
  data$interests %in% c("Blockchain", "Entrepreneurship", "Event Management", "Teaching") ~ "Other",
  data$interests == "Networking" ~ "Infrastructure",
  data$interests == "Hardware" ~ "Hardware",
  TRUE ~ "Other"
)

data$interests_group <- factor(data$interests_group)

# -------- Create unified skill groups -----------
data$skills_group <- case_when(
  data$skills %in% c("Graphic Design", "Photography", "Video Editing") ~ "Design",
  data$skills %in% c("Data Entry", "AI", "Machine Learning", "Database", "Market Analysis", "System Analysis") ~ "Data_and_AI",
  data$skills %in% c("App Development", "Frontend", "Python", "Web Development", "Programming") ~ "Programming",
  data$skills %in% c("Communication", "Teaching", "Problem Solving", "E-Commerce", "Mentoring", "Content Creation") ~ "Other",
  data$skills == "Cyber Security" ~ "Cyber",
  data$skills == "Networking" ~ "Infrastructure",
  TRUE ~ "Other"
)

data$skills_group <- factor(data$skills_group)

cat("\nFrequency of interests_group:\n")

table(data$interests_group)
cat("\nFrequency of skills_group:\n")

table(data$skills_group)
```

# feature engineering:

```{r}
# -------------------- Constructing Composite Features for Academic Engagement --------------------

data <- data %>%
  mutate(
    # 1. Study_Attendance_Interaction: combines hours * normalized attendance
    Study_Attendance_Interaction = as.numeric(study_hours) * (attendance  / 100),
    
    # 2. Study_Efficiency: study hours per session
    Study_Efficiency = ifelse(study_freq == 0, 0, study_hours / study_freq),
    
    # 3. Total Study Effort Score: combines all together
    TotalStudyEffort = (as.numeric(study_hours) * (attendance / 100)) +
                       ifelse(teacher_consult == "Yes", 1, 0) +
                       ifelse(co_curricular == "Yes", 1, 0))


# Count each condition separately
cat("\nCheck how much Values have in income colum that bigger than 200,000:\n")
sum(data$income > 200000, na.rm = TRUE)         # Over 200,000 income
cat("\nCheck how much Values have in cgpa colum that smaller than 1.2:\n")
sum(data$cgpa < 1.2, na.rm = TRUE)              # CGPA below 1.2


outlier_counts <- data %>%
  summarise(
    Over10_AttendanceInteraction = sum(Study_Attendance_Interaction > 10, na.rm = TRUE),
    Over10_StudyEfficiency = sum(Study_Efficiency > 10, na.rm = TRUE),
    Over10_TotalEffort = sum(TotalStudyEffort > 11, na.rm = TRUE)
  )

#print(outlier_counts)

# -------------------- Filter out outliers --------------------

data_clean <- data %>%
  filter(income <= 200000 | is.na(income)) %>%
  filter(cgpa >= 1.2 | is.na(cgpa)) %>%
  filter(
    Study_Attendance_Interaction <= 11 | is.na(Study_Attendance_Interaction),
    Study_Efficiency <= 11 | is.na(Study_Efficiency),
    TotalStudyEffort <= 11 | is.na(TotalStudyEffort)
  )

# -------------------- Add more features that represent CGPA and more feature engineering --------------------

# Add scaled CGPA (0–100)
data <- data %>%
  mutate(cgpa_scaled = cgpa * 25)

data_clean <- data_clean %>%
  mutate(cgpa_scaled = cgpa * 25)

# Add classified CGPA (quartiles A-D)
data <- data %>%
  mutate(cgpa_class = ntile(cgpa, 4)) %>%
  mutate(cgpa_class = recode_factor(as.factor(cgpa_class),
                                    `1` = "D",
                                    `2` = "C",
                                    `3` = "B",
                                    `4` = "A")) %>%
  mutate(income_log = log(income + 1)) %>%
  mutate(english_level = case_when(
  english == "Basic" ~ 1,
  english == "Intermediate" ~ 2,
  english == "Advanced" ~ 3
  ))


data_clean <- data_clean %>%
  mutate(cgpa_class = ntile(cgpa, 4)) %>%
  mutate(cgpa_class = recode_factor(as.factor(cgpa_class),
                                    `1` = "D",
                                    `2` = "C",
                                    `3` = "B",
                                    `4` = "A")) %>%
  mutate(income_log = log(income + 1)) %>%
  mutate(english_level = case_when(
  english == "Basic" ~ 1,
  english == "Intermediate" ~ 2,
  english == "Advanced" ~ 3
  ))
```

# Target feature (CGPA) distribution:

The distribution of CGPA is **left-skewed** and exhibits a sharp peak at **exactly 3.0**.
While most students cluster between **3.0 and 4.0**, there are very few with low CGPA (\< 2.0).

```{r, echo=FALSE, warning=FALSE	}
# -------------------------------------------------------------
# Plot distributions of CGPA (raw, scaled, and categorical)
# -------------------------------------------------------------

# Histogram of raw CGPA (0–4 scale)
ggplot(data_clean, aes(x = cgpa)) +
  geom_histogram(fill = "steelblue", bins = 30, color = "black") +
  labs(title = "Distribution of Raw CGPA",
       x = "CGPA (0–4 scale)", y = "Count") +
  theme_minimal()

```

# General question - Can a student's final academic performance be predicted based on different groups of features such as demographic characteristics, academic history, and learning habits?

Let’s start by examining the model results with all three feature groups included, and then remove each group in turn to see how it individually affects the prediction accuracy.

## Model Results Table:

| \# | Dataset variant | Target column | Best model | Test-set metrics | Top-3 predictors (importance) | Key comment |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 1 | Full Data | `cgpa` | **Random-Forest Reg.** | **R² 0.871 · RMSE 0.300 · MAE 0.228** | `sgpa` 0.44 › `credits` 0.19 › `semester` 0.10 | Strongest regression results overall |
| 2 | Full Data | `cgpa_scaled` | **Random-Forest Reg.** | R² 0.871 · RMSE 7.49 · MAE 5.70 | same order as Row 1 | Scaling to 0–100 range changes only magnitude |
| 3 | Cleaned Data | `cgpa` | **Random-Forest Reg.** | R² 0.656 · RMSE 0.280 · MAE 0.208 | `sgpa` 0.20 › `semester` 0.02 › `probation` 0.02 | Lower R² due to trimmed variance |
| 4 | Cleaned Data | `cgpa_scaled` | **Random-Forest Reg.** | R² 0.656 · RMSE 6.99 · MAE 5.20 | same order as Row 3 | Matches pattern from Row 3 |
| 5 | Full Data | `cgpa_class` (A–D) | **Random-Forest Clf.** | **Acc 0.704 · F1 0.705** | `sgpa` 0.22 › `income_log` 0.02 › `semester` 0.02 | Most confusion between B and C |
| 6 | Cleaned Data | `cgpa_class` (A–D) | **Random-Forest Clf.** | **Acc 0.737 · F1 0.737** | `sgpa` 0.21 › `credits` 0.03 › `income_log` 0.03 | Cleaning improved class accuracy slightly |

## Insights Summary:

The clearest pattern is that **Random Forest models** consistently outperform linear models for both regression and classification tasks.
They achieve **higher R² values** (up to \~0.87) and **lower RMSE/MAE**, confirming their capacity to capture nonlinear relationships in the data.

1.  **Data filtering**: When applying filtering criteria to remove students with **extreme income levels (\>200K)**, we observe a **drop in regression accuracy** (e.g., R² drops from 0.87 → 0.66).\
    However, **classification performance improves slightly** (+3% accuracy), likely because noisy cases around grade boundaries (A–D) were removed.

2.  **Target column transformation**: Scaling `cgpa` to `cgpa_scaled` (×25) does not change the model fit – only the error units grow proportionally.
    Classification into letter grades (`cgpa_class`) shows decent separation with \~0.74 F1, even in multi-class setting.

3.  **Model type**: Random Forest Regression outperformed Linear Regression by a large margin in all settings – improving R² by \~0.15–0.20 and reducing RMSE by \~20–25%.\
    In classification, only Random Forest was tested and achieved solid accuracy (\~0.73–0.74) despite 4-class complexity.

4.  **Feature importance**: Across all tasks, **previous GPA (`sgpa`) dominates** model performance – accounting for 20–45% of importance.\
    `credits`, `semester`, `income_log`, and `probation_Yes` also show moderate influence.\
    In contrast, many **behavioral features** such as `study_hours`, \`teacher_consult\`\` had **low predictive value**, both in RF and linear models.

5.  **Key takeaways**:

    -   For predicting CGPA, using the **full dataset** is preferred unless classification robustness is the main goal.\
    -   `sgpa` and academic structure (`credits`, `semester`) are by far the strongest signals.\
    -   **Demographic features** (such as `age`, `gender`, `income_log`, `english level`, `disability`, `living_with`) showed **low to moderate influence**, with `income_log` being the most relevant among them.\
    -   **Learning behavior variables** (like `study_hours`, `study_freq`, `teacher_consult`, `co_curricular`) had **low importance** across all models – suggesting limited explanatory power in the presence of prior GPA.

We plan to **analyze each group of features separately** (academic, demographic, behavioral) in the next phase to assess whether CGPA can be effectively predicted using only one group at a time.

```{r, echo=FALSE, warning=FALSE	}
# ================================================
# GLOBAL: Create list to collect feature importances
# ================================================
all_importances <- list()

# ================================================
# MAIN FUNCTION: Run modeling pipeline
# ================================================
run_model_analysis <- function(data, target_col, task_type = c("regression", "classification"), dataset_name = "") {
  task_type <- match.arg(task_type)
  
  # ----------------------------------------
  # 1. Data splitting: 80/20 train/test with stratification
  # ----------------------------------------
  set.seed(123)
  data_split <- initial_split(data, prop = 0.8, strata = !!sym(target_col))
  train_data <- training(data_split)
  test_data <- testing(data_split)
  
  # ----------------------------------------
  # 2. Create 5-fold cross-validation on training set
  # ----------------------------------------
  set.seed(123)
  cv_folds <- vfold_cv(train_data, v = 5, strata = !!sym(target_col))

  # ----------------------------------------
  # 3. Define predictors (features to use) and categorical variables
  # ----------------------------------------

  predictors <- c("study_freq","study_hours", "co_curricular","Study_Attendance_Interaction","Study_Efficiency","TotalStudyEffort",
                  "teacher_consult", "learning_mode", "attendance","sgpa ","probation", "suspension","scholarship", "semester", "credits",
                  "age ","gender", "income_log","living_with","english", "health_issues", "disability","relationship")

  cat_vars <- c( "learning_mode", "teacher_consult","co_curricular", "probation", "suspension","scholarship", "gender", "english","health_issues", "disability","relationship","living_with")
                
  # ----------------------------------------
  # 4. Build preprocessing recipe
  # ----------------------------------------
  model_recipe <- recipe(as.formula(paste(target_col, "~", paste(predictors, collapse = "+"))), data = train_data) %>%
    step_mutate_at(cat_vars, fn = as.factor) %>%
    step_nzv(all_predictors()) %>%
    step_corr(all_numeric_predictors(), threshold = 0.9) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors())

  # ----------------------------------------
  # 5. Define models based on task type (regression/classification)
  # ----------------------------------------
  
  if (task_type == "regression") {
    models <- list(
      "Linear Regression" = linear_reg() %>% set_engine("lm"),
      "Random Forest Regression" = rand_forest(mode = "regression", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  } else {
    models <- list(
      "Random Forest Classifier" = rand_forest(mode = "classification", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  }
  
  # ================================================
  # 6. Train and evaluate each model
  # ================================================
  for (model_name in names(models)) {
    cat("\n\U0001F539 Running:", model_name, "on target:", target_col, "with dataset:", dataset_name, "\n")

    wflow <- workflow() %>%
      add_model(models[[model_name]]) %>%
      add_recipe(model_recipe)

    # ----------------------------------------
    # 6.1 Cross-validation loop (manual implementation)
    # ----------------------------------------
    cv_preds <- vector("list", length = 5)
    for (i in seq_along(cv_folds$splits)) {
      split <- cv_folds$splits[[i]]
      train_fold <- analysis(split)
      test_fold <- assessment(split)
      model_fit <- fit(wflow, data = train_fold)
      preds <- predict(model_fit, new_data = test_fold, type = ifelse(task_type == "classification", "class", "numeric")) %>%
        bind_cols(test_fold %>% select(all_of(target_col)))
      cv_preds[[i]] <- preds
    }
    all_cv_preds <- bind_rows(cv_preds)
    
    
    # ----------------------------------------
    # 6.2 Print cross-validation performance
    # ----------------------------------------
    cat("\n\U0001F4C8 Cross-validation performance (manual):\n")
    if (task_type == "regression") {
      r1 <- rmse(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      cat(sprintf("RMSE: %.4f | R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, r3$.estimate))
    } else {
      a <- accuracy(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
    }

    # ----------------------------------------
    # 6.3 Final model training on full train set
    # ----------------------------------------
    final_fit <- fit(wflow, data = train_data)
    
    # ----------------------------------------
    # 6.4 Predict on test set and print test performance
    # ----------------------------------------
    test_pred <- predict(final_fit, new_data = test_data, type = ifelse(task_type == "classification", "class", "numeric")) %>%
      bind_cols(test_data %>% select(all_of(target_col)))

    cat("\n\U0001F4CA Test set performance:\n")
    if (task_type == "regression") {
      r1 <- rmse(test_pred, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(test_pred, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(test_pred, truth = !!sym(target_col), estimate = .pred)
      adj_r2 <- 1 - (1 - r2$.estimate) * ((nrow(test_data) - 1) / (nrow(test_data) - length(predictors) - 1))
      cat(sprintf("RMSE: %.4f | R²: %.4f | Adjusted R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, adj_r2, r3$.estimate))
    } else {
      a <- accuracy(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
      cat("\n📉 Confusion Matrix:\n")
      print(conf_mat(test_pred, truth = !!sym(target_col), estimate = .pred_class))
    }
    
    
    # ----------------------------------------
    # 6.5 Extract and store feature importance or model coefficients
    # ----------------------------------------
    model_fit <- extract_fit_parsnip(final_fit)
    
    if (inherits(model_fit$fit, "lm") || inherits(model_fit$fit, "glm")) {
      coef_df <- broom::tidy(model_fit$fit) %>%
        mutate(model = model_name, target = target_col, dataset = dataset_name)
      print(coef_df)
      all_importances[[length(all_importances) + 1]] <- coef_df
      
    } else if (!is.null(model_fit$fit$variable.importance)) {
      imp_tbl <- enframe(model_fit$fit$variable.importance, name = "feature", value = "importance") %>%
        arrange(desc(importance)) %>%
        mutate(model = model_name, target = target_col, dataset = dataset_name)
      
      print(imp_tbl)
      all_importances[[length(all_importances) + 1]] <- imp_tbl
      
      # 📊 Plot feature importance only for Random Forest
      if (inherits(model_fit$fit, "ranger")) {
        p <- ggplot(imp_tbl[1:10, ], aes(x = reorder(feature, importance), y = importance)) +
          geom_col(fill = "darkgreen") +
          coord_flip() +
          labs(
            title = paste("Top Features:", model_name),
            subtitle = paste("Target:", target_col, "| Data:", dataset_name),
            x = "Feature", y = "Importance"
          ) +
          theme_minimal()
        print(p)
      }
      
    } else {
      cat("\n⚠️ Feature importance not available for model:", model_name, "\n")
    }
  }
}


# ================================================
# 7. Define dataset versions to run on
# ================================================
all_data_variants <- list(
  "Full Data - Regr" = data %>% select(-cgpa_class),
  "Cleaned Data - Regr" = data_clean %>% select(-cgpa_class),
  "Full Data - Class" = data,
  "Cleaned Data - Class" = data_clean
)


# ================================================
# 8. Define target outcomes and types
# ================================================
targets <- list(
  list(col = "cgpa", type = "regression", dataset = "Full Data - Regr"),
  list(col = "cgpa_scaled", type = "regression", dataset = "Full Data - Regr"),
  list(col = "cgpa", type = "regression", dataset = "Cleaned Data - Regr"),
  list(col = "cgpa_scaled", type = "regression", dataset = "Cleaned Data - Regr"),
  list(col = "cgpa_class", type = "classification", dataset = "Full Data - Class"),
  list(col = "cgpa_class", type = "classification", dataset = "Cleaned Data - Class")
)


# ================================================
# 9. Loop through each modeling configuration
# ================================================
for (target in targets) {
  dname <- target$dataset
  cat("\n=============================\n")
  cat("Dataset:", dname, "| Target:", target$col, "| Type:", target$type, "\n")
  cat("=============================\n")
  run_model_analysis(all_data_variants[[dname]], target$col, task_type = target$type, dataset_name = dname)
}

```

# q1 - Can the final GPA be predicted based on prior academic records at the university?

## Analysis of predictive features:

Detailed description of academic records variable distributions:

-   The bar plot of **suspension** shows that the vast majority of students have not been suspended, with very few reporting suspension.

-   The histogram of **SGPA** (Semester Grade Point Average) indicates that most students have GPAs concentrated between 2.0 and 3.5, with a peak around 2.7.

-   The **credits** distribution reveals a bimodal pattern, with a large group of students having earned very few credits and another spread across the full credit range up to 150.

-   The **semester** distribution shows that most students are in early semesters (1–10), with very few continuing beyond semester 15.

-   The **scholarship** bar plot shows more students without scholarships than with, but the gap is not extremely wide.

-   The **probation** graph shows that while most students are not on probation, a significant portion (about one-third) has experienced it.

```{r, echo=FALSE, warning=FALSE	}

# Distribution of numerical educational variables: histograms and summary stats
edu_quant_vars <- c("sgpa", "semester", "credits")

for (var in edu_quant_vars) {
  cat("\n")
  print(paste("Histogram and Summary for:", var))
  print(summary(data[[var]]))
  plt <- ggplot(data, aes_string(x = var)) +
    geom_histogram(fill = "skyblue", color = "black", bins = 15) +
    labs(title = paste("Distribution of", var), x = var, y = "Count") +
    theme_minimal()
  
  print(plt)
}

# Distribution of categorical educational variables: frequency tables and bar plots
edu_cat_vars <- c("probation", "suspension", "scholarship")

for (var in edu_cat_vars) {
  cat("\n")
  print(paste("Frequency table for:", var))
  print(table(data[[var]]))
  plt <- ggplot(data, aes_string(x = var)) +
    geom_bar(fill = "lightgreen", color = "black") +
    labs(title = paste("Bar Plot of", var), x = var, y = "Count") +
    theme_minimal()
  
  print(plt)
}

```

## Correlation analysis between features and with the target:

**Strong Positive Correlations:** - `semester` ↔ `credits`: r = 0.90 — expected progression (more semesters → more credits) - `cgpa` ↔ `sgpa`: r = 0.65 — past GPA is a strong predictor of current GPA - `sgpa` ↔ `scholarship`: r = 0.40 — scholarships awarded to higher-performing students

**Strong Negative Correlations:** - `scholarship` ↔ `credits`: r = -0.41 - `scholarship` ↔ `semester`: r = -0.38\
→ Suggests scholarships tend to go to early-stage students - `cgpa` ↔ `probation`: r = -0.33\
→ Academic warnings strongly tied to lower GPA

**Additional Notes:** - `credits` and `semester` are tightly coupled; either can represent academic progress.
- `suspension` shows weaker correlation overall; rare in data.

**Takeaway:** - Strongest predictor of CGPA is clearly SGPA.
- Academic flags (probation) are associated with lower GPA.

```{r, warning=FALSE	}
# ============================================================================
# Academic Variables Correlation Matrix
# ============================================================================

# ============================================================================
# 1. Define Variables and Prepare Data
# ============================================================================

# Selected academic variables
selected_vars <- c("cgpa", "sgpa", "probation", "suspension", 
                   "scholarship", "semester", "credits")
analysis_data<- data_clean
# Check data availability
if(!exists("analysis_data") || is.null(analysis_data)) {
  if(exists("train_clean")) {
    analysis_data <- train_clean
  } else if(exists("train_prep")) {
    analysis_data <- train_prep
  } else if(exists("train_data")) {
    analysis_data <- train_data[!is.na(train_data$cgpa), ]
  } else {
    cat("lease run the data cleaning code first\n")
    stop("No data available")
  }
}

# Check variable availability
existing_vars <- selected_vars[selected_vars %in% names(analysis_data)]
missing_vars <- selected_vars[!selected_vars %in% names(analysis_data)]

#cat("Selected academic variables:\n")
for(i in 1:length(selected_vars)) {
  status <- ifelse(selected_vars[i] %in% existing_vars, "✅", "❌")
  #cat(sprintf("%s %d. %s\n", status, i, selected_vars[i]))
}

if(length(missing_vars) > 0) {
  cat("\nMissing variables in data:\n")
  for(var in missing_vars) {
    cat("-", var, "\n")
  }
}

# ============================================================================
# 2. Prepare Data for Correlation Analysis
# ============================================================================

if(length(existing_vars) >= 2) {
  
  #cat(sprintf("\nWorking with %d existing variables\n", length(existing_vars)))
  
  # Select relevant data
  correlation_data <- analysis_data[, existing_vars, drop = FALSE]
  
  # Handle categorical variables - convert to numeric
  #cat("\nConverting categorical variables to numeric:\n")
  
  for(var in existing_vars) {
    if(is.factor(correlation_data[[var]])) {
      
      if(length(levels(correlation_data[[var]])) == 2) {
        # Binary variables: convert to 0/1 (No=0, Yes=1)
        correlation_data[[var]] <- as.numeric(correlation_data[[var]]) - 1
        #cat(sprintf("- %s: binary (No=0, Yes=1)\n", var))
        
      } else {
        # Other categorical variables
        correlation_data[[var]] <- as.numeric(correlation_data[[var]])
        #cat(sprintf("- %s: basic numeric conversion\n", var))
      }
    } else if(is.numeric(correlation_data[[var]])) {
      #cat(sprintf("- %s: already numeric\n", var))
    }
  }
  
  # Remove missing data
  correlation_data <- correlation_data[complete.cases(correlation_data), ]
  
  #cat(sprintf("\nSample size after cleaning: %d observations\n", nrow(correlation_data)))
  #cat(sprintf("Final variables: %s\n", paste(names(correlation_data), collapse = ", ")))
  
  # ============================================================================
  # 3. Calculate Correlation Matrix
  # ============================================================================
  
  if(nrow(correlation_data) > 20 && ncol(correlation_data) >= 2) {
    
    #cat("\nCalculating correlation matrix...\n")
    
    # Calculate correlation matrix
    corr_matrix <- cor(correlation_data, use = "complete.obs")
    
    cat("\n", paste(rep("=", 60), collapse = ""))
    cat("\nPearson Correlation Matrix")
    cat("\n", paste(rep("=", 60), collapse = ""))
    cat("\n")
    print(round(corr_matrix, 3))
    
    # ============================================================================
    # 4. Identify Strong Correlations
    # ============================================================================
    
    cat("\nStrong correlations (|r| ≥ 0.3):\n")
    cat(paste(rep("-", 55), collapse = ""), "\n")
    
    strong_correlations <- data.frame(
      Var1 = character(),
      Var2 = character(),
      Correlation = numeric(),
      Strength = character(),
      stringsAsFactors = FALSE
    )
    
    # Find strong correlations
    for(i in 1:(nrow(corr_matrix)-1)) {
      for(j in (i+1):ncol(corr_matrix)) {
        corr_val <- corr_matrix[i,j]
        if(abs(corr_val) >= 0.3) {
          
          strength <- if(abs(corr_val) >= 0.7) "Very Strong" else
                     if(abs(corr_val) >= 0.5) "Strong" else "Moderate"
          
          strong_correlations <- rbind(strong_correlations, data.frame(
            Var1 = rownames(corr_matrix)[i],
            Var2 = colnames(corr_matrix)[j],
            Correlation = corr_val,
            Strength = strength
          ))
        }
      }
    }
    
    if(nrow(strong_correlations) > 0) {
      # Sort by correlation strength
      strong_correlations <- strong_correlations[order(abs(strong_correlations$Correlation), decreasing = TRUE), ]
      
      cat(sprintf("%-12s %-12s %12s %15s\n", "Variable 1", "Variable 2", "Correlation", "Strength"))
      cat(paste(rep("-", 55), collapse = ""), "\n")
      
      for(i in 1:nrow(strong_correlations)) {
        row <- strong_correlations[i, ]
        cat(sprintf("%-12s %-12s %12.3f %15s\n", 
                    row$Var1, row$Var2, row$Correlation, row$Strength))
      }
    } else {
      cat("No strong correlations found (|r| ≥ 0.3)\n")
    }
    
    # ============================================================================
    # 5. Correlations with CGPA
    # ============================================================================
    
    if("cgpa" %in% names(correlation_data)) {
      cat("\nCorrelations with CGPA (sorted by strength):\n")
      cat(paste(rep("-", 50), collapse = ""), "\n")
      
      cgpa_correlations <- corr_matrix[, "cgpa"]
      cgpa_correlations <- cgpa_correlations[names(cgpa_correlations) != "cgpa"]
      cgpa_correlations <- sort(cgpa_correlations, decreasing = TRUE)
      
      cat(sprintf("%-15s %12s %18s\n", "Variable", "Correlation", "Strength"))
      cat(paste(rep("-", 50), collapse = ""), "\n")
      
      for(i in 1:length(cgpa_correlations)) {
        var_name <- names(cgpa_correlations)[i]
        corr_val <- cgpa_correlations[i]
        
        strength <- if(abs(corr_val) >= 0.7) "Very Strong" else
                    if(abs(corr_val) >= 0.5) "Strong" else
                    if(abs(corr_val) >= 0.3) "Moderate" else
                    if(abs(corr_val) >= 0.1) "Weak" else "Negligible"
        
        cat(sprintf("%-15s %12.3f %18s\n", var_name, corr_val, strength))
      }
    }
    
    # ============================================================================
    # 6. Create Correlation Matrix Plot
    # ============================================================================
    
    cat("\nCreating correlation matrix visualization...\n")
    
    # Set up plot
    par(mfrow = c(1, 1), mar = c(10, 10, 4, 6))
    
    # Color palette
    colors <- colorRampPalette(c("#d73027", "#f46d43", "#fdae61", "#fee08b", 
                               "#e6f598", "#abdda4", "#66c2a5", "#3288bd", "#5e4fa2"))(100)
    
    # Create heatmap
    image(1:ncol(corr_matrix), 1:nrow(corr_matrix), 
          t(corr_matrix[nrow(corr_matrix):1, ]),
          col = colors,
          xlab = "", ylab = "",
          main = "Correlation Matrix - Academic Variables",
          axes = FALSE,
          zlim = c(-1, 1))
    
    # Add axis labels
    axis(1, at = 1:ncol(corr_matrix), 
         labels = colnames(corr_matrix), las = 2, cex.axis = 0.9)
    axis(2, at = 1:nrow(corr_matrix), 
         labels = rownames(corr_matrix)[nrow(corr_matrix):1], 
         las = 2, cex.axis = 0.9)
    
    # Add correlation values
    for(i in 1:nrow(corr_matrix)) {
      for(j in 1:ncol(corr_matrix)) {
        text_color <- if(abs(corr_matrix[i,j]) > 0.5) "white" else "black"
        
        text(j, nrow(corr_matrix) - i + 1, 
             round(corr_matrix[i,j], 2), 
             cex = 0.8, 
             col = text_color,
             font = 2)
      }
    }
    
    # Add color legend
    legend("right", 
           legend = c("1.0", "0.5", "0.0", "-0.5", "-1.0"),
           fill = colors[c(100, 75, 50, 25, 1)],
           title = "Correlation",
           cex = 0.8,
           inset = c(-0.2, 0),
           xpd = TRUE)
    
    # Add grid lines
    abline(h = (0:nrow(corr_matrix)) + 0.5, col = "white", lwd = 0.5)
    abline(v = (0:ncol(corr_matrix)) + 0.5, col = "white", lwd = 0.5)
    
    # Reset plot margins
    par(mar = c(5, 4, 4, 2))
    
    # ============================================================================
    # 7. Summary and Academic Insights
    # ============================================================================
    
    cat("\n", paste(rep("=", 60), collapse = ""))
    cat("\nSummary and Academic Insights")
    cat("\n", paste(rep("=", 60), collapse = ""))
    
    cat(sprintf("\nAnalysis Summary:\n"))
    cat(sprintf("• Academic variables analyzed: %d\n", ncol(corr_matrix)))
    cat(sprintf("• Sample size: %d observations\n", nrow(correlation_data)))
    cat(sprintf("• Strong correlations found: %d\n", nrow(strong_correlations)))
    
    # Find strongest correlation overall
    if(nrow(strong_correlations) > 0) {
      strongest <- strong_correlations[1, ]
      cat(sprintf("• Strongest correlation: %s ↔ %s (r = %.3f)\n", 
                  strongest$Var1, strongest$Var2, strongest$Correlation))
    }
    
    # Best CGPA predictor
    if("cgpa" %in% names(correlation_data)) {
      best_predictor_idx <- which.max(abs(cgpa_correlations))
      best_predictor <- names(cgpa_correlations)[best_predictor_idx]
      best_corr <- cgpa_correlations[best_predictor_idx]
      
      cat(sprintf("• Best CGPA predictor: %s (r = %.3f)\n", best_predictor, best_corr))
    }
    
    cat("\nAcademic Performance Insights:\n")
    
    # SGPA-CGPA relationship
    if("sgpa" %in% names(correlation_data) && "cgpa" %in% names(correlation_data)) {
      sgpa_cgpa_corr <- corr_matrix["sgpa", "cgpa"]
      cat(sprintf("• SGPA-CGPA consistency: r = %.3f (%s)\n", 
                  sgpa_cgpa_corr,
                  ifelse(sgpa_cgpa_corr > 0.7, "very high consistency", 
                        ifelse(sgpa_cgpa_corr > 0.5, "good consistency", "moderate consistency"))))
    }
    
    # Academic warnings analysis
    if(any(c("probation", "suspension") %in% names(correlation_data))) {
      cat("• Academic warning variables included in analysis\n")
      
      if("probation" %in% names(correlation_data) && "cgpa" %in% names(correlation_data)) {
        prob_corr <- corr_matrix["probation", "cgpa"]
        cat(sprintf("  - Probation-CGPA: r = %.3f (%s relationship)\n", 
                    prob_corr, ifelse(prob_corr < -0.1, "negative", "minimal")))
      }
      
      if("suspension" %in% names(correlation_data) && "cgpa" %in% names(correlation_data)) {
        susp_corr <- corr_matrix["suspension", "cgpa"]
        cat(sprintf("  - Suspension-CGPA: r = %.3f (%s relationship)\n", 
                    susp_corr, ifelse(susp_corr < -0.1, "negative", "minimal")))
      }
    }
    
    # Scholarship analysis
    if("scholarship" %in% names(correlation_data) && "cgpa" %in% names(correlation_data)) {
      schol_corr <- corr_matrix["scholarship", "cgpa"]
      cat(sprintf("• Scholarship-CGPA: r = %.3f (%s)\n", 
                  schol_corr,
                  ifelse(schol_corr > 0.2, "scholarships linked to high performance", 
                        ifelse(schol_corr > 0.1, "weak positive link", "minimal relationship"))))
    }
    
    # Academic progress analysis
    if(any(c("semester", "credits") %in% names(correlation_data))) {
      cat("• Academic progress indicators:\n")
      
      if("semester" %in% names(correlation_data) && "cgpa" %in% names(correlation_data)) {
        sem_corr <- corr_matrix["semester", "cgpa"]
        cat(sprintf("  - Semester-CGPA: r = %.3f (%s)\n", 
                    sem_corr, ifelse(abs(sem_corr) > 0.1, "semester effect detected", "no clear semester trend")))
      }
      
      if("credits" %in% names(correlation_data) && "cgpa" %in% names(correlation_data)) {
        cred_corr <- corr_matrix["credits", "cgpa"]
        cat(sprintf("  - Credits-CGPA: r = %.3f (%s)\n", 
                    cred_corr, ifelse(cred_corr > 0.1, "more credits linked to higher GPA", 
                                     ifelse(cred_corr < -0.1, "potential academic load effect", "no clear credit effect"))))
      }
    }
    
    # Variable encoding notes
    cat("\nVariable Encoding Notes:\n")
    binary_vars <- c("probation", "suspension", "scholarship")
    found_binary <- intersect(binary_vars, names(correlation_data))
    
    if(length(found_binary) > 0) {
      cat(sprintf("• Binary variables (%s): No=0, Yes=1\n", paste(found_binary, collapse = ", ")))
    }
    
    if("sgpa" %in% names(correlation_data)) {
      cat("• SGPA: Previous semester GPA (continuous scale)\n")
    }
    
    if(any(c("semester", "credits") %in% names(correlation_data))) {
      cat("• Academic progress: Semester and credits as continuous variables\n")
    }
    
    #cat("\n✅ Academic correlation analysis completed successfully!\n")
    
  } else {
    cat("❌ Insufficient data for reliable correlation analysis\n")
    cat(sprintf("Need at least 20 observations and 2 variables\n"))
    cat(sprintf("Current: %d observations, %d variables\n", 
                nrow(correlation_data), ncol(correlation_data)))
  }
  
} else {
  cat("❌ Insufficient variables found in dataset\n")
  cat("Need at least 2 variables for correlation analysis\n")
}

# Reset plot settings
par(mfrow = c(1, 1), mar = c(5, 4, 4, 2))


```

## Relationship analysis with the target variable:

-   **SGPA → CGPA:**\
    A clear positive trend is observed: students in higher SGPA groups consistently achieve higher CGPAs.

-   **Semester → CGPA:**\
    No clear trend across CGPA groups.
    While the median semester is fairly stable, the “Low” CGPA group shows much higher variance and outliers.

-   **Credits → CGPA:**\
    The relationship is weak.
    Median credits are similar across groups, but again, “Low” CGPA students show greater spread.
    A mild upward trend: CGPA tends to rise with the number of credits completed.
    However, the differences between groups are modest.

-   **Probation → CGPA:**\
    Students on probation have lower CGPAs on average.
    The boxplot shows a noticeable drop in median CGPA among those Students that on probation.

-   **Suspension → CGPA:**\
    A similar pattern to probation: suspended students tend to have lower CGPA, though the sample size of suspension students smaller and variance is high.

-   **Scholarship → CGPA:**\
    Students who received scholarships tend to have higher CGPAs.

**Overall:**\
SGPA remains the most informative and consistent predictor of CGPA.
Administrative flags like probation and scholarship also reflect meaningful differences in academic standing.
Other variables (semester, credits) show noisier or indirect effects.

```{r, echo=FALSE, warning=FALSE	}
# ============================================================================
# Relationship Between Predictors and CGPA (Continuous on Y-axis)
# ============================================================================
# This section visualizes how CGPA (Y-axis) varies across groups of academic predictors.
# Grouping is done by:
# - Discretized SGPA, semester, and credits (4 bins each)
# - Binary indicators (probation, suspension, scholarship)
# Each plot shows how CGPA distribution shifts across these grouped variables.


# Remove NA values
filtered_data <- data %>%
  filter(!is.na(sgpa), !is.na(cgpa))

# Discretize sgpa into 4 equal-width bins
filtered_data <- filtered_data %>%
  mutate(sgpa_group = cut(sgpa,
                          breaks = 4,
                          labels = c("Low", "Medium", "High", "Very High"),
                          include.lowest = TRUE))

# Boxplot of CGPA by sgpa group
ggplot(filtered_data, aes(x = sgpa_group, y = cgpa, fill = sgpa_group)) +
  geom_boxplot() +
  labs(title = "CGPA by SGPA Group",
       x = "SGPA Group",
       y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")


# Boxplot
ggplot(data, aes(x = probation, y = cgpa)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "CGPA by Probation Status", x = "Probation", y = "CGPA") +
  theme_minimal()

# T-test (for binary group comparison)
# t_test_result <- t.test(cgpa ~ probation, data = data)
# print(t_test_result)


# ------------------ suspension vs CGPA ------------------

# Boxplot to compare CGPA across suspension groups
ggplot(data, aes(x = suspension, y = cgpa)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "CGPA by Suspension Status", x = "Suspension", y = "CGPA") +
  theme_minimal()

# T-test to compare CGPA means between suspended vs. not suspended students
# t_test_result <- t.test(cgpa ~ suspension, data = data)
# print(t_test_result)


# ------------------ scholarship vs CGPA ------------------

# Boxplot to compare CGPA across scholarship groups
ggplot(data, aes(x = scholarship, y = cgpa)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "CGPA by Scholarship Status", x = "Scholarship", y = "CGPA") +
  theme_minimal()

# T-test to compare CGPA means between scholarship vs. no-scholarship students
# t_test_result <- t.test(cgpa ~ scholarship, data = data)
# print(t_test_result)


# Remove NA values
filtered_data <- data %>%
  filter(!is.na(semester), !is.na(cgpa))

# Discretize semester into 4 equal-width bins
filtered_data <- filtered_data %>%
  mutate(semester_group = cut(semester,
                              breaks = 4,
                              labels = c("Early", "Mid", "Advanced", "Final"),
                              include.lowest = TRUE))

# Boxplot of CGPA by semester group
ggplot(filtered_data, aes(x = semester_group, y = cgpa, fill = semester_group)) +
  geom_boxplot() +
  labs(title = "CGPA by Semester Group",
       x = "Semester Group",
       y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")


# Remove NA values
filtered_data <- data %>%
  filter(!is.na(credits), !is.na(cgpa))

# Discretize credits into 4 equal-width bins
filtered_data <- filtered_data %>%
  mutate(credits_group = cut(credits,
                             breaks = 4,
                             labels = c("Few", "Moderate", "Many", "Very Many"),
                             include.lowest = TRUE))

# Boxplot of CGPA by credits group
ggplot(filtered_data, aes(x = credits_group, y = cgpa, fill = credits_group)) +
  geom_boxplot() +
  labs(title = "CGPA by Credits Group",
       x = "Credits Group",
       y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")

```

```{r, echo=FALSE, warning=FALSE	}
# ============================================================================
# Relationship Between Predictors and CGPA (Categorical on X-axis)
# ============================================================================
# This section visualizes how academic predictors vary across CGPA quartile groups (Low–Excellent).
# CGPA is treated as a categorical variable (X-axis), and we examine:
# - Boxplots for continuous predictors (e.g., SGPA, semester, credits)
# - Barplots for binary flags (e.g., probation, suspension, scholarship) 


# Filter CGPA in desired range
filtered_data <- data %>%
  filter(!is.na(cgpa)) %>%
  filter(cgpa >= 1.5, cgpa <= 4)

# Create 4 balanced quartile groups
filtered_data <- filtered_data %>%
  mutate(cgpa_group = ntile(cgpa, 4)) %>%
  mutate(cgpa_group = factor(cgpa_group,
                             levels = 1:4,
                             labels = c("Low", "Average", "Good", "Excellent")))

# Learning-related variables by type
numeric_vars <- c("sgpa", "semester", "credits")
binary_vars <- c("probation", "suspension", "scholarship")

# 1. Create filtered_data with cgpa_group
filtered_data <- data %>%
  filter(!is.na(cgpa), cgpa >= 1.5, cgpa <= 4) %>%
  mutate(cgpa_group = ntile(cgpa, 4)) %>%
  mutate(cgpa_group = factor(cgpa_group,
                             levels = 1:4,
                             labels = c("Low", "Average", "Good", "Excellent")))

# 2. Boxplots for numeric variables
plot_list_num <- lapply(numeric_vars, function(var) {
  ggplot(filtered_data, aes_string(x = "cgpa_group", y = var, fill = "cgpa_group")) +
    geom_boxplot() +
    labs(title = paste("Distribution of", var, "by CGPA Group"),
         x = "CGPA Group", y = var) +
    theme_minimal() +
    theme(legend.position = "none")
})

# 3. Barplots for binary variables (side-by-side layout)
plot_list_bin <- lapply(binary_vars, function(var) {
  ggplot(filtered_data, aes_string(x = var, fill = "cgpa_group")) +
    geom_bar(position = "dodge") +
    labs(title = paste("Distribution of", var, "by CGPA Group"),
         x = var, y = "Count") +
    theme_minimal()
})

# 4. Display the plots
for (p in plot_list_num) { print(p) }
for (p in plot_list_bin) { print(p) }

```

## Insights and references from previous research:

Several studies support the idea that final GPA can be effectively predicted using prior academic records within the university.
- Yağcı (2022) demonstrated that midterm exam grades are the most influential factor in predicting final exam scores, achieving \~74% accuracy using Random Forest.
- Asif et al. (2017) found that early-year course grades alone—without demographic data—can predict graduation outcomes with over 90% accuracy.
- Similarly, Beaulac and Rosenthal (2019) showed that first-year grades, especially in quantitatively challenging departments, are strong predictors of graduation success, with an AUC of 0.88.
- Ationu (2025) confirmed that past grades significantly outweigh demographic data, with attendance adding moderate value to prediction models.
- Finally, another study by Asif et al. (2017) focused on CS undergraduates and found that early-semester grades are the strongest predictors of final CGPA, with models reaching over 80% accuracy.
Collectively, these studies consistently show that early academic performance is the most powerful predictor of final GPA.

previous research:

-   Yağcı M. (2022) – *Educational Data Mining: Prediction of Students’ Academic Performance Using ML Algorithms*

1.  **Title / Authors / Year** Educational Data Mining: Prediction of Students’ Academic Performance Using ML Algorithms – Yağcı M.

    (2022) 

2.  **Paper link** <https://slejournal.springeropen.com/articles/10.1186/s40561-022-00192-z> :contentReference[oaicite:0]{index="0"}

3.  **Data link** Dataset provided as Springer Nature figshare supplementary file\
    <https://figshare.com/articles/dataset/19304996>

4.  **Sample & how collected** 1,854 first-year students in “Turkish I” (winter 2019-2020); mid-term and final grades automatically exported from SIS (no questionnaire)

5.  **Research question** Can mid-semester information predict the final-exam grade?

6.  **Models & top result** RF, k-NN, SVM, LR, NB → **Random-Forest accuracy ≈ 74 %** (10-fold CV)

7.  **Conclusions / feature insights** Mid-term score overwhelmingly dominates importance; department / faculty adds only marginal lift

8.  **Feature quote** “*Students’ mid-term-exam grades are an important predictor to be used in predicting their final-exam grades.*” :contentReference[oaicite:1]{index="1"}

------------------------------------------------------------------------

-   Asif R. et al. (2017) – *Analyzing Undergraduate Students’ Performance Using Educational Data Mining*

1.  **Title / Authors / Year** Analyzing Undergraduate Students’ Performance Using Educational Data Mining – Asif R.
    et al. (2017)

2.  **Paper link** <https://www.sciencedirect.com/science/article/pii/S0360131517301124> :contentReference[oaicite:2]{index="2"}

3.  **Data link** Not publicly available (internal university registry)

4.  **Sample & how collected** \> 50 000 course records from IT undergraduates, cohorts 2007-2014; raw marks dumped from registrar DB (no surveys)

5.  **Research question** How early can we identify students likely to graduate successfully vs. fail?

6.  **Models & top result** C4.5, JRip, k-NN, NB, RF → **C4.5 accuracy 92 %** (train on cohort A, test on cohort B)

7.  **Conclusions / feature insights** Five first-/second-year “indicator” courses explain most variance; adding demographics gave no gain

8.  **Feature quotes** “*Only admission marks and marks of first- and second-year courses are used; no socio-economic or demographic features are considered.*” :contentReference[oaicite:3]{index="3"}

------------------------------------------------------------------------

-   Beaulac C. & Rosenthal J. (2019) – *Early-Course Grades Predict University Graduation*

1.  **Title / Authors / Year** Early-Course Grades Predict University Graduation – Beaulac C.
    & Rosenthal J.

    (2019) 

2.  **Paper link** <https://arxiv.org/abs/1802.03418> :contentReference[oaicite:4]{index="4"}

3.  **Data link** Not publicly available (University of Toronto internal extract)

4.  **Sample & how collected** 65 000 first-year entrants (2009-2013); administrative grades merged with graduation outcome (no survey)

5.  **Research question** How well do first-year grades predict degree completion and eventual major?

6.  **Models & top result** Random-Forest vs. logistic regression → **RF AUC 0.88** for graduation

7.  **Conclusions / feature insights** Grades in low-grading departments (Math, Economics, Finance) plus first-year seminar enrollment are strongest; demographics weak

8.  **Feature quote** “*Grades in Mathematics (MAT), Finance (COMPG) and Economics (ECO) are consistently among the most important grade variables.*” :contentReference[oaicite:5]{index="5"}

------------------------------------------------------------------------

-   Ationu H. (2025) – *Predicting Student Performance Using Machine Learning: A Data-Driven Approach with Consideration of Special-Needs Students*

1.  **Title / Authors / Year** Predicting Student Performance Using Machine Learning: A Data-Driven Approach with Consideration of Special-Needs Students – Ationu H.

    (2025) 

2.  **Paper link** <https://www.researchgate.net/publication/391874377_Predicting_student_performance_using_machine_learning_a_data-driven_approach_with_consideration_of_special_needs_students> :contentReference[oaicite:4]{index="4"}

3.  **Data link** Not publicly available (institutional MIS exports)

4.  **Sample & how collected** 1,236 undergraduates (University of Wolverhampton, 2021-2023); academic histories + attendance logs; special-needs flag from disability office (administrative records)

5.  **Research question** Compare ML models for final-score prediction while explicitly accounting for special-needs students

6.  **Models & top result** DT, RF, SVM, MLP → **Random-Forest MAE ≈ 0.38** (MLP similar)

7.  **Conclusions / feature insights** Past grades dominate; attendance adds ≈ 12 % importance; demographics add ≈ 4 %

8.  **Feature quote** “*While past evaluations substantially influence students’ performance, a comprehensive analysis reveals that several additional factors, such as attendance rates, parents’ occupations and education levels...also contribute to academic success.*” :contentReference[oaicite:7]{index="7"}

------------------------------------------------------------------------

-   Asif R. et al. (2017) – *Predicting Academic Performance of CS Undergraduates*

1.  **Title / Authors / Year** Predicting Academic Performance of CS Undergraduates – Asif R.
    et al. (2017)

2.  **Paper link** <https://www.researchgate.net/publication/317109378_Analyzing_undergraduate_students%27_performance_using_educational_data_mining> :contentReference[oaicite:8]{index="8"}

3.  **Data link** Not publicly available (Student-Information System)

4.  **Sample & how collected** 287 CS majors (batch 2013-2016) at a private Pakistani university; course marks and attendance logs exported from SIS

5.  **Research question** Can early-year performance predict final CGPA, and which features matter most?

6.  **Models & top result** J48, RF, NB, SVM, k-NN (10-fold CV) → **J48 & RF accuracy \> 80 %**

7.  **Conclusions / feature insights** Early-semester course grades are top predictors; attendance moderate; demographics weak

8.  **Feature quote** “*The results indicate that by focusing on a small number of courses that are indicators of particularly good or poor performance, it is possible to provide timely warning and support to low-achieving students, and advice and opportunities to high-performing students.*” :contentReference[oaicite:9]{index="9"}

## Model implementation and evaluation:

| \# | Dataset variant | Target column | Best model | Test-set metrics | Top-3 predictors (importance) | Key comment |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 1 | Full Data | `cgpa` | **Random-Forest Reg.** | **R² 0.925 · RMSE 0.216 · MAE 0.143** | `sgpa` 0.70 › `credits` 0.23 › `semester` 0.11 | Strongest overall performance; probation & scholarship almost zero weight |
| 2 | Full Data | `cgpa_scaled` | **Random-Forest Reg.** | R² 0.925 · RMSE 5.41 · MAE 3.59 | same order as Row 1 | Scaling by ×25 changes only units, not fit |
| 3 | Cleaned Data | `cgpa` | **Random-Forest Reg.** | **R² 0.734 · RMSE 0.242 · MAE 0.150** | `sgpa` 0.35 › `semester` 0.11 › `probation` 0.01 | Variance trimmed by filters → lower R² but still good |
| 4 | Cleaned Data | `cgpa_scaled` | **Random-Forest Reg.** | R² 0.734 · RMSE 6.05 · MAE 3.76 | same order as Row 3 | Same pattern, different scale |
| 5 | Full Data | `cgpa_class` (A–D) | **Random-Forest Clf.** | **Acc 0.758 · F1 0.758** | `sgpa` 0.43 › `semester` 0.08 › `probation` 0.01 | Errors mainly B ↔ C; demographics weak |
| 6 | Cleaned Data | `cgpa_class` (A–D) | **Random-Forest Clf.** | **Acc 0.776 · F1 0.776** | `sgpa` 0.44 › `credits` 0.13 › `semester` 0.09 | Cleaning boosts accuracy ≈ +1.8 pp |

-   Modeling Summary – Key Insights:

1.  **Best Overall Model:** Random-Forest Regression on **Full Data** predicting raw `cgpa` (Row 1)\
    → Achieved **R² = 0.925**, **RMSE = 0.216**, **MAE = 0.143**

2.  **Model Comparison:**\
    Random Forest consistently outperformed Linear Regression across all regression runs, improving R² by \~0.15 and reducing RMSE by 20–35%.

3.  **Filtered vs. Full Data:**\
    Filtering out students with extreme income and low CGPA reduced regression accuracy (**R² dropped to 0.734**), likely due to trimmed variance.\
    However, classification performance slightly improved (\~+1.8 pp accuracy), as outliers had blurred the A–D grade boundaries.

4.  **`cgpa` vs. `cgpa_scaled`:**\
    Predictive performance was identical.
    Scaling `cgpa` by ×25 changed only the error units (RMSE/MAE), not model fit.
    Slight advantage: more intuitive understanding of score ranges (0–100 instead of 0–4)

5.  **Feature Importance (across all RF models):**

    -   `sgpa` is the dominant signal (35–70% of total importance)\
    -   `credits` and `semester` provide moderate lift\
    -   `probation` and `scholarship` contribute almost nothing (near-zero importance; inconsistent direction in linear models)

```{r, echo=FALSE, warning=FALSE}
# ================================================
# GLOBAL: Create list to collect feature importances
# ================================================
all_importances <- list()

# ================================================
# MAIN FUNCTION: Run modeling pipeline
# ================================================
run_model_analysis <- function(data, target_col, task_type = c("regression", "classification"), dataset_name = "") {
  task_type <- match.arg(task_type)
  
  # ----------------------------------------
  # 1. Data splitting: 80/20 train/test with stratification
  # ----------------------------------------
  set.seed(123)
  data_split <- initial_split(data, prop = 0.8, strata = !!sym(target_col))
  train_data <- training(data_split)
  test_data <- testing(data_split)
  
  # ----------------------------------------
  # 2. Create 5-fold cross-validation on training set
  # ----------------------------------------
  set.seed(123)
  cv_folds <- vfold_cv(train_data, v = 5, strata = !!sym(target_col))

  # ----------------------------------------
  # 3. Define predictors (features to use) and categorical variables
  # ----------------------------------------

  predictors <- c("sgpa ","probation", "suspension",
                  "scholarship", "semester", "credits")

  cat_vars <- c( "probation", "suspension","scholarship")
                
  # ----------------------------------------
  # 4. Build preprocessing recipe
  # ----------------------------------------
  model_recipe <- recipe(as.formula(paste(target_col, "~", paste(predictors, collapse = "+"))), data = train_data) %>%
    step_mutate_at(cat_vars, fn = as.factor) %>%
    step_nzv(all_predictors()) %>%
    step_corr(all_numeric_predictors(), threshold = 0.9) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors())

  # ----------------------------------------
  # 5. Define models based on task type (regression/classification)
  # ----------------------------------------
  
  if (task_type == "regression") {
    models <- list(
      "Linear Regression" = linear_reg() %>% set_engine("lm"),
      "Random Forest Regression" = rand_forest(mode = "regression", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  } else {
    models <- list(
      "Random Forest Classifier" = rand_forest(mode = "classification", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  }
  
  # ================================================
  # 6. Train and evaluate each model
  # ================================================
  for (model_name in names(models)) {
    cat("\n\U0001F539 Running:", model_name, "on target:", target_col, "with dataset:", dataset_name, "\n")

    wflow <- workflow() %>%
      add_model(models[[model_name]]) %>%
      add_recipe(model_recipe)

    # ----------------------------------------
    # 6.1 Cross-validation loop (manual implementation)
    # ----------------------------------------
    cv_preds <- vector("list", length = 5)
    for (i in seq_along(cv_folds$splits)) {
      split <- cv_folds$splits[[i]]
      train_fold <- analysis(split)
      test_fold <- assessment(split)
      model_fit <- fit(wflow, data = train_fold)
      preds <- predict(model_fit, new_data = test_fold, type = ifelse(task_type == "classification", "class", "numeric")) %>%
        bind_cols(test_fold %>% select(all_of(target_col)))
      cv_preds[[i]] <- preds
    }
    all_cv_preds <- bind_rows(cv_preds)
    
    
    # ----------------------------------------
    # 6.2 Print cross-validation performance
    # ----------------------------------------
    cat("\n\U0001F4C8 Cross-validation performance (manual):\n")
    if (task_type == "regression") {
      r1 <- rmse(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      cat(sprintf("RMSE: %.4f | R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, r3$.estimate))
    } else {
      a <- accuracy(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
    }

    # ----------------------------------------
    # 6.3 Final model training on full train set
    # ----------------------------------------
    final_fit <- fit(wflow, data = train_data)
    
    # ----------------------------------------
    # 6.4 Predict on test set and print test performance
    # ----------------------------------------
    test_pred <- predict(final_fit, new_data = test_data, type = ifelse(task_type == "classification", "class", "numeric")) %>%
      bind_cols(test_data %>% select(all_of(target_col)))

    cat("\n\U0001F4CA Test set performance:\n")
    if (task_type == "regression") {
      r1 <- rmse(test_pred, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(test_pred, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(test_pred, truth = !!sym(target_col), estimate = .pred)
      adj_r2 <- 1 - (1 - r2$.estimate) * ((nrow(test_data) - 1) / (nrow(test_data) - length(predictors) - 1))
      cat(sprintf("RMSE: %.4f | R²: %.4f | Adjusted R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, adj_r2, r3$.estimate))
    } else {
      a <- accuracy(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
      cat("\n📉 Confusion Matrix:\n")
      print(conf_mat(test_pred, truth = !!sym(target_col), estimate = .pred_class))
    }
    
    
    # ----------------------------------------
    # 6.5 Extract and store feature importance or model coefficients
    # ----------------------------------------
    model_fit <- extract_fit_parsnip(final_fit)
    if (inherits(model_fit$fit, "lm") || inherits(model_fit$fit, "glm")) {
      coef_df <- broom::tidy(model_fit$fit) %>%
        mutate(model = model_name, target = target_col, dataset = dataset_name)
      print(coef_df)
      all_importances[[length(all_importances) + 1]] <- coef_df
    } else if (!is.null(model_fit$fit$variable.importance)) {
      imp_tbl <- enframe(model_fit$fit$variable.importance, name = "feature", value = "importance") %>%
        arrange(desc(importance)) %>%
        mutate(model = model_name, target = target_col, dataset = dataset_name)
      print(imp_tbl)
      all_importances[[length(all_importances) + 1]] <- imp_tbl
    } else {
      cat("\nFeature importance not available for model:", model_name, "\n")
    }
  }
}


# ================================================
# 7. Define dataset versions to run on
# ================================================
all_data_variants <- list(
  "Full Data - Regr" = data %>% select(-cgpa_class),
  "Cleaned Data - Regr" = data_clean %>% select(-cgpa_class),
  "Full Data - Class" = data,
  "Cleaned Data - Class" = data_clean
)


# ================================================
# 8. Define target outcomes and types
# ================================================
targets <- list(
  list(col = "cgpa", type = "regression", dataset = "Full Data - Regr"),
  list(col = "cgpa_scaled", type = "regression", dataset = "Full Data - Regr"),
  list(col = "cgpa", type = "regression", dataset = "Cleaned Data - Regr"),
  list(col = "cgpa_scaled", type = "regression", dataset = "Cleaned Data - Regr"),
  list(col = "cgpa_class", type = "classification", dataset = "Full Data - Class"),
  list(col = "cgpa_class", type = "classification", dataset = "Cleaned Data - Class")
)


# ================================================
# 9. Loop through each modeling configuration
# ================================================
for (target in targets) {
  dname <- target$dataset
  cat("\n=============================\n")
  cat("Dataset:", dname, "| Target:", target$col, "| Type:", target$type, "\n")
  cat("=============================\n")
  run_model_analysis(all_data_variants[[dname]], target$col, task_type = target$type, dataset_name = dname)
}
```

## Results analysis and conclusions

The analysis confirms that a student’s cumulative GPA (CGPA) can be predicted with high precision from records already held by the university.
Descriptive analysis shows that most students have an SGPA close to 2.7, credits split into two main groups (early-stage and late-stage students), and suspensions are very rare.
Correlation inspection reinforced intuitive academic dynamics: semester and credits progress in lock-step (*r* ≈ 0.90), while SGPA exhibits the strongest linear tie to CGPA (*r* ≈ 0.65).
Disciplinary flags behave as expected—probation correlates negatively with CGPA, whereas scholarships align positively—yet their overall explanatory power is small relative to prior grades.

Modeling results sharpen these impressions.
Random-Forest regression on the full dataset attains an **R² of 0.925 and RMSE of 0.216**, vastly outperforming linear benchmarks and preserving accuracy after rescaling the target.
Cleaning extreme observations reduces variance and R² (0.734) but slightly lifts classification accuracy by clarifying grade-band boundaries.
Across every run, SGPA accounts for more than a third—and up to two-thirds—of total feature importance, with credits and semester providing incremental lift and administrative variables proving largely negligible.

| Variant   | Task              | Best Model                        |
|-----------|-------------------|-----------------------------------|
| Full data | CGPA (regression) | RF, **R² 0.925 / RMSE 0.216**     |
| Cleaned   | CGPA class (A–D)  | RF, **Accuracy 0.776 / F1 0.776** |

These findings mirror earlier studies—Yağcı (2022), Asif et al. (2017), and Beaulac & Rosenthal (2019)—that highlight early-term grades as the dominant signal while demographic or disciplinary factors add little marginal value.
Our results therefore reinforce the consensus that monitoring semester-level performance is both sufficient and efficient for forecasting degree outcomes.

**Recommended next steps**

1.  **Deploy an early-warning dashboard** that flags students whose SGPA trajectories predict CGPAs below the institutional benchmark; the trained Random-Forest model can supply live, semester-by-semester risk scores.\
2.  **Institute rigorous, centralised grade logging**—ensure every course mark and semester SGPA is captured promptly and uniformly to support more reliable forecasting and enable curriculum adjustments for students trending toward low CGPAs.

Based on our findings and prior literature, early academic performance remains the most reliable indicator of eventual degree success, underscoring the value of systematic monitoring and prompt, data-driven interventions.

# q2 - Can the final GPA be predicted based on demographic characteristics?

## Analysis of predictive features:

Detailed description of demographic variable distributions

-   The histogram of **age** shows a sharp concentration around 20–22 years, with the median at 21.
    Very few students are younger than 19 or older than 25.

-   The histogram of **income** (log-transformed) is right-skewed, with most students reporting low to moderate income levels.
    A few high-income values are still present.

-   The **english** bar plot shows that most students rate their proficiency as “Intermediate” (670), followed by “Basic” (304) and “Advance” (220).
    This plot reflects the data **after filtering out income values above 100,000**.

-   The **gender** distribution is slightly male-dominant, with 672 male vs. 522 female students.

-   The **living_with** variable shows a near-even split, with a slight majority living with family (642 vs. 552 living independently).

-   The **health_issues** bar plot reveals that most students (987) report no health conditions, but a substantial minority (207) do, a factor that may impact academic stability.

-   The **disability** distribution is highly imbalanced: only 27 students reported having a disability, compared to 1167 who did not.

-   The **relationship** status is strongly skewed toward “Single” (896), followed by smaller groups in a relationship (187), married (96), and engaged (15).
    This aligns with a typical young academic population.

```{r, echo=FALSE, warning=FALSE}
# Define demographic variables
quant_vars <- c("age")  # Numeric variables
cat_vars <- c("english", "gender", "living_with", "health_issues", "disability", "relationship")  # Categorical variables

# 1. Histograms for numeric variables
for (var in quant_vars) {
  cat("\n")
  print(paste("Histogram and Summary for:", var))
  print(summary(data[[var]]))
  
  plt <- ggplot(data, aes_string(x = var)) +
    geom_histogram(fill = "skyblue", color = "black", bins = 15) +
    labs(title = paste("Distribution of", var), x = var, y = "Count") +
    theme_minimal()
  
  print(plt)
}

# 2. Barplots for categorical variables
for (var in cat_vars) {
  cat("\n")
  print(paste("Frequency table for:", var))
  print(table(data[[var]]))
  
  plt <- ggplot(data, aes_string(x = var)) +
    geom_bar(fill = "lightgreen", color = "black") +
    labs(title = paste("Bar Plot of", var), x = var, y = "Count") +
    theme_minimal()
  
  print(plt)
}

# Filter out extreme outliers above 100,000
filtered_data <- data[data$income <= 100000, ]

# Draw histogram for filtered income with readable X-axis
hist(filtered_data$income,
     breaks = 50,
     col = "lightblue",
     border = "white",
     main = "Histogram of Income (Filtered < 100,000)",
     xlab = "Income",
     ylab = "Count",
     axes = FALSE)


  
# Add nicer axes
axis(1, at = pretty(filtered_data$income), labels = pretty(filtered_data$income), las = 2)
axis(2)
box()

cat("\n")
print("Histogram and Summary for: income_log")
print(summary(data$income_log))
```

## Correlation analysis between features and with the target:

**Weak Correlations with CGPA:** - `english` → `cgpa`: r = 0.10 — slight positive link; higher English level very weakly associated with better GPA.
- `living_with` → `cgpa`: r = 0.08 — negligible advantage for students living with family.
- `relationship` → `cgpa`: r = -0.10 — students in relationships tend to have slightly lower GPAs.
- All other demographic features (e.g., `age`, `gender`, `income`, `health_issues`, `disability`) have **correlations near zero** (\|r\| \< 0.06), suggesting no meaningful effect on GPA.

**Notable Correlations Between Demographics:** - `health_issues` ↔ `disability`: r = 0.22 — students with health issues are more likely to report disabilities.
- `income` ↔ `relationship`: r = 0.22 — higher income levels associated with relationship status.
- `gender` ↔ `income`: r = -0.15 — males tend to report higher income than females.
- `age` ↔ `relationship`: r = 0.14 — older students more likely to be in relationships.

**Additional Notes:** - No demographic feature reached even a moderate correlation (\|r\| ≥ 0.3) with any other — relationships are all weak at best.

**Takeaway:**\
- Demographic features do **not appear to be predictive of academic performance** in this dataset.\
- The learning environment appears equitable: GPA outcomes do not vary significantly by age, gender, income, health, or living status.

```{r, echo=FALSE, warning=FALSE}
# ============================================================================
# Demographic Variables Correlation Matrix - Clean Version
# ============================================================================

# ============================================================================
# 1. Define Variables and Prepare Data
# ============================================================================
analysis_data<-data_clean
# Selected demographic variables
selected_vars <- c("cgpa", "age", "gender", "income", "living_with", 
                   "english", "health_issues", "disability", "relationship")

# Check data availability
if(!exists("analysis_data") || is.null(analysis_data)) {
  if(exists("train_clean")) {
    analysis_data <- train_clean
  } else if(exists("train_prep")) {
    analysis_data <- train_prep
  } else if(exists("train_data")) {
    analysis_data <- train_data[!is.na(train_data$cgpa), ]
  } else {
    cat("❌ Please run the data cleaning code first\n")
    stop("No data available")
  }
}

# Check variable availability
existing_vars <- selected_vars[selected_vars %in% names(analysis_data)]
missing_vars <- selected_vars[!selected_vars %in% names(analysis_data)]

#cat("Selected variables:\n")
for(i in 1:length(selected_vars)) {
  status <- ifelse(selected_vars[i] %in% existing_vars, "✅", "❌")
  #cat(sprintf("%s %d. %s\n", status, i, selected_vars[i]))
}

if(length(missing_vars) > 0) {
  #cat("\n⚠ Missing variables in data:\n")
  for(var in missing_vars) {
    #cat("-", var, "\n")
  }
}

# ============================================================================
# 2. Prepare Data for Correlation Analysis
# ============================================================================

if(length(existing_vars) >= 2) {
  
  #cat(sprintf("\nWorking with %d existing variables\n", length(existing_vars)))
  
  # Select relevant data
  correlation_data <- analysis_data[, existing_vars, drop = FALSE]
  
  # Handle categorical variables - convert to numeric
  #cat("\nConverting categorical variables to numeric:\n")
  
  for(var in existing_vars) {
    if(is.factor(correlation_data[[var]])) {
      
      if(var == "english" && length(levels(correlation_data[[var]])) == 3) {
        # English proficiency: Basic=1, Intermediate=2, Advance=3
        correlation_data[[var]] <- as.numeric(factor(correlation_data[[var]], 
                                                   levels = c("Basic", "Intermediate", "Advance"),
                                                   ordered = TRUE))
        #cat(sprintf("- %s: ordered scale (1=Basic, 2=Intermediate, 3=Advance)\n", var))
        
      } else if(var == "relationship" && length(levels(correlation_data[[var]])) > 2) {
        # Relationship status: Single=1, In a relationship=2, Engaged=3, Married=4
        correlation_data[[var]] <- as.numeric(factor(correlation_data[[var]], 
                                                   levels = c("Single", "In a relationship", "Engaged", "Married"),
                                                   ordered = TRUE))
        #cat(sprintf("- %s: ordered scale (1=Single, 2=Relationship, 3=Engaged, 4=Married)\n", var))
        
      } else if(length(levels(correlation_data[[var]])) == 2) {
        # Binary variables: convert to 0/1
        correlation_data[[var]] <- as.numeric(correlation_data[[var]]) - 1
        #cat(sprintf("- %s: binary (0/1)\n", var))
        
      } else {
        # Other categorical variables
        correlation_data[[var]] <- as.numeric(correlation_data[[var]])
        #cat(sprintf("- %s: basic numeric conversion\n", var))
      }
    }
  }
  
  # Remove missing data
  correlation_data <- correlation_data[complete.cases(correlation_data), ]
  
  
  # ============================================================================
  # 3. Calculate Correlation Matrix
  # ============================================================================
  
  if(nrow(correlation_data) > 20 && ncol(correlation_data) >= 2) {
    

    # Calculate correlation matrix
    corr_matrix <- cor(correlation_data, use = "complete.obs")
    
    cat("\n", paste(rep("=", 60), collapse = ""))
    cat("\nPearson Correlation Matrix")
    cat("\n", paste(rep("=", 60), collapse = ""))
    cat("\n")
    print(round(corr_matrix, 3))
    
    # ============================================================================
    # 4. Identify Strong Correlations
    # ============================================================================
    
    cat("\nStrong correlations (|r| ≥ 0.3):\n")
    cat(paste(rep("-", 50), collapse = ""), "\n")
    
    strong_found <- FALSE
    
    for(i in 1:(nrow(corr_matrix)-1)) {
      for(j in (i+1):ncol(corr_matrix)) {
        corr_val <- corr_matrix[i,j]
        if(abs(corr_val) >= 0.3) {
          
          strength <- if(abs(corr_val) >= 0.7) "Very Strong" else
                     if(abs(corr_val) >= 0.5) "Strong" else "Moderate"
          
          cat(sprintf("%-12s %-12s %8.3f %-15s\n", 
                      rownames(corr_matrix)[i], colnames(corr_matrix)[j], 
                      corr_val, strength))
          strong_found <- TRUE
        }
      }
    }
    
    if(!strong_found) {
      cat("No strong correlations found (|r| ≥ 0.3)\n")
    }
    
    # ============================================================================
    # 5. Correlations with CGPA
    # ============================================================================
    
    if("cgpa" %in% names(correlation_data)) {
      cat("\nCorrelations with CGPA (sorted by strength):\n")
      cat(paste(rep("-", 45), collapse = ""), "\n")
      
      cgpa_correlations <- corr_matrix[, "cgpa"]
      cgpa_correlations <- cgpa_correlations[names(cgpa_correlations) != "cgpa"]
      cgpa_correlations <- sort(cgpa_correlations, decreasing = TRUE)
      
      cat(sprintf("%-15s %10s %15s\n", "Variable", "Correlation", "Strength"))
      cat(paste(rep("-", 45), collapse = ""), "\n")
      
      for(i in 1:length(cgpa_correlations)) {
        var_name <- names(cgpa_correlations)[i]
        corr_val <- cgpa_correlations[i]
        
        strength <- if(abs(corr_val) >= 0.7) "Very Strong" else
                    if(abs(corr_val) >= 0.5) "Strong" else
                    if(abs(corr_val) >= 0.3) "Moderate" else
                    if(abs(corr_val) >= 0.1) "Weak" else "Negligible"
        
        cat(sprintf("%-15s %10.3f %15s\n", var_name, corr_val, strength))
      }
    }
    
    # ============================================================================
    # 6. Create Correlation Matrix Plot
    # ============================================================================
    
    cat("\n📈 Creating correlation matrix visualization...\n")
    
    # Set up plot
    par(mfrow = c(1, 1), mar = c(10, 10, 4, 6))
    
    # Color palette
    colors <- colorRampPalette(c("#d73027", "#f46d43", "#fdae61", "#fee08b", 
                               "#e6f598", "#abdda4", "#66c2a5", "#3288bd", "#5e4fa2"))(100)
    
    # Create heatmap
    image(1:ncol(corr_matrix), 1:nrow(corr_matrix), 
          t(corr_matrix[nrow(corr_matrix):1, ]),
          col = colors,
          xlab = "", ylab = "",
          main = "Correlation Matrix - Demographic Variables",
          axes = FALSE,
          zlim = c(-1, 1))
    
    # Add axis labels
    axis(1, at = 1:ncol(corr_matrix), 
         labels = colnames(corr_matrix), las = 2, cex.axis = 0.9)
    axis(2, at = 1:nrow(corr_matrix), 
         labels = rownames(corr_matrix)[nrow(corr_matrix):1], 
         las = 2, cex.axis = 0.9)
    
    # Add correlation values
    for(i in 1:nrow(corr_matrix)) {
      for(j in 1:ncol(corr_matrix)) {
        text_color <- if(abs(corr_matrix[i,j]) > 0.5) "white" else "black"
        
        text(j, nrow(corr_matrix) - i + 1, 
             round(corr_matrix[i,j], 2), 
             cex = 0.8, 
             col = text_color,
             font = 2)
      }
    }
    
    # Add color legend
    legend("right", 
           legend = c("1.0", "0.5", "0.0", "-0.5", "-1.0"),
           fill = colors[c(100, 75, 50, 25, 1)],
           title = "Correlation",
           cex = 0.8,
           inset = c(-0.2, 0),
           xpd = TRUE)
    
    # Add grid lines
    abline(h = (0:nrow(corr_matrix)) + 0.5, col = "white", lwd = 0.5)
    abline(v = (0:ncol(corr_matrix)) + 0.5, col = "white", lwd = 0.5)
    
    # Reset plot margins
    par(mar = c(5, 4, 4, 2))
    
    # ============================================================================
    # 7. Summary and Key Insights
    # ============================================================================
    
    cat("\n", paste(rep("=", 60), collapse = ""))
    cat("\nSummary and Key Insights")
    cat("\n", paste(rep("=", 60), collapse = ""))
    
    cat(sprintf("\nAnalysis Summary:\n"))
    cat(sprintf("• Variables analyzed: %d\n", ncol(corr_matrix)))
    cat(sprintf("• Sample size: %d observations\n", nrow(correlation_data)))
    
    # Find strongest correlation overall
    max_corr <- 0
    max_var1 <- ""
    max_var2 <- ""
    
    for(i in 1:(nrow(corr_matrix)-1)) {
      for(j in (i+1):ncol(corr_matrix)) {
        if(abs(corr_matrix[i,j]) > max_corr) {
          max_corr <- abs(corr_matrix[i,j])
          max_var1 <- rownames(corr_matrix)[i]
          max_var2 <- colnames(corr_matrix)[j]
        }
      }
    }
    
    if(max_corr > 0) {
      cat(sprintf("• Strongest overall correlation: %s ↔ %s (|r| = %.3f)\n", 
                  max_var1, max_var2, max_corr))
    }
    
    # Best CGPA predictor
    if("cgpa" %in% names(correlation_data)) {
      best_predictor_idx <- which.max(abs(cgpa_correlations))
      best_predictor <- names(cgpa_correlations)[best_predictor_idx]
      best_corr <- cgpa_correlations[best_predictor_idx]
      
      cat(sprintf("• Best CGPA predictor: %s (r = %.3f)\n", best_predictor, best_corr))
    }
    
    cat("\nKey Insights:\n")
    
    # Age analysis
    if("age" %in% names(correlation_data) && "cgpa" %in% names(correlation_data)) {
      age_cgpa_corr <- corr_matrix["age", "cgpa"]
      cat(sprintf("• Age effect: r = %.3f (%s)\n", 
                  age_cgpa_corr, 
                  ifelse(abs(age_cgpa_corr) > 0.1, "some age influence", "minimal age effect")))
    }
    
    # Income analysis
    if("income" %in% names(correlation_data) && "cgpa" %in% names(correlation_data)) {
      income_cgpa_corr <- corr_matrix["income", "cgpa"]
      cat(sprintf("• Economic effect: r = %.3f (%s)\n", 
                  income_cgpa_corr,
                  ifelse(abs(income_cgpa_corr) > 0.1, "economic influence detected", "minimal economic effect")))
    }
    
    # Variable conversion notes
    if("english" %in% names(correlation_data)) {
      cat("• English proficiency treated as ordered variable (1-3 scale)\n")
    }
    
    if("relationship" %in% names(correlation_data)) {
      cat("• Relationship status treated as ordered variable (1-4 scale)\n")
    }
    
    if("gender" %in% names(correlation_data)) {
      cat("• Gender converted to binary variable for analysis\n")
    }
    

  } else {
    cat("nsufficient data for reliable correlation analysis\n")
    cat(sprintf("Need at least 20 observations and 2 variables\n"))
    cat(sprintf("Current: %d observations, %d variables\n", 
                nrow(correlation_data), ncol(correlation_data)))
  }
  
} else {
  cat("sufficient variables found in dataset\n")
  cat("Need at least 2 variables for correlation analysis\n")
}

# Reset plot settings
par(mfrow = c(1, 1), mar = c(5, 4, 4, 2))



```

## Relationship analysis with the target variable:

Interpretation of demographic distributions by CGPA group

-   **Age → CGPA:**\
    Slightly older students (25+) are more frequent in the “Low” CGPA group.
    For all other CGPA groups, the age distribution is fairly uniform (20–23).

-   **Income → CGPA:**\
    No consistent pattern between income groups and CGPA.\
    All CGPA groups are concentrated around similar income levels, and high-income students are spread across all outcomes.

-   **Gender → CGPA:**\
    Gender distributions are relatively balanced across CGPA groups.\
    Both genders are evenly represented, and performance appears comparable.

-   **Living Situation → CGPA:**\
    Students living with family appear more in “Good” and “Excellent” CGPA groups.
    Those living independently are more common in the “Low” group.
    This suggests that family support may offer a slight academic benefit.

-   **English Proficiency → CGPA:**\
    Students with “Advance” English are overrepresented in the “Excellent” CGPA group.
    “Intermediate” students are the largest group overall.
    A mild trend suggests better English skills are helpful for academic success.

-   **Health Issues → CGPA:**\
    Students with health issues are slightly more prevalent in the “Low” and “Good” CGPA groups.
    Still, distributions are relatively similar overall.
    Health status shows minor association with CGPA.

-   **Disability → CGPA:**\
    Students with disabilities are very few, and they appear across all CGPA groups.
    Due to small sample size, no strong conclusions can be drawn.

-   **Relationship Status → CGPA:**\
    Married students are more common in the “Low” CGPA group.\
    Most other students (especially in higher CGPA groups) are single.

**Overall:**\
- None of the demographic variables show strong separation between CGPA groups.\
- This supports the earlier conclusion that academic performance is mostly independent of demographic background.

```{r, echo=FALSE, warning=FALSE}
# ============================================================================
# Relationship Between Demographics and CGPA (Continuous on Y-axis)
# ============================================================================
# This section visualizes how CGPA (Y-axis) varies across demographic groups.
# Each plot shows CGPA distribution across:
# - Discretized variables (age, income_log) split into 4 equal-width bins
# - Binary/categorical variables (gender, living_with, english, health_issues,
#   disability, relationship)
# This helps assess how CGPA shifts across different demographic segments.

# Remove NA values
filtered_data <- data %>%
  filter(!is.na(age), !is.na(cgpa))

# Discretize age into 4 equal-width bins
filtered_data <- filtered_data %>%
  mutate(age_group = cut(age,
                         breaks = 4,
                         labels = c("Young", "Mid-Young", "Mid-Old", "Older"),
                         include.lowest = TRUE))

# Boxplot of CGPA by age group
ggplot(filtered_data, aes(x = age_group, y = cgpa, fill = age_group)) +
  geom_boxplot() +
  labs(title = "CGPA by Age Group",
       x = "Age Group",
       y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")

filtered_data <- data_clean %>%
  filter(!is.na(income), !is.na(cgpa)) %>%
  mutate(income_group = cut(income,
                            breaks = 4,
                            labels = c("Low", "Mid-Low", "Mid-High", "High"),
                            include.lowest = TRUE))

ggplot(filtered_data, aes(x = income_group, y = cgpa, fill = income_group)) +
  geom_boxplot() +
  labs(title = "CGPA by Income Group",
       x = "Income Group", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")

filtered_data <- data %>% filter(!is.na(gender), !is.na(cgpa))

ggplot(filtered_data, aes(x = gender, y = cgpa, fill = gender)) +
  geom_boxplot() +
  labs(title = "CGPA by Gender",
       x = "Gender", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")


filtered_data <- data %>% filter(!is.na(living_with), !is.na(cgpa))

ggplot(filtered_data, aes(x = living_with, y = cgpa, fill = living_with)) +
  geom_boxplot() +
  labs(title = "CGPA by Living Situation",
       x = "Living With", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")


filtered_data <- data %>% filter(!is.na(english), !is.na(cgpa))

ggplot(filtered_data, aes(x = english, y = cgpa, fill = english)) +
  geom_boxplot() +
  labs(title = "CGPA by English Proficiency",
       x = "English Level", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")


filtered_data <- data %>% filter(!is.na(health_issues), !is.na(cgpa))

ggplot(filtered_data, aes(x = health_issues, y = cgpa, fill = health_issues)) +
  geom_boxplot() +
  labs(title = "CGPA by Health Issues",
       x = "Health Issues", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")

filtered_data <- data %>% filter(!is.na(disability), !is.na(cgpa))

ggplot(filtered_data, aes(x = disability, y = cgpa, fill = disability)) +
  geom_boxplot() +
  labs(title = "CGPA by Disability Status",
       x = "Disability", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")

filtered_data <- data %>% filter(!is.na(relationship), !is.na(cgpa))

ggplot(filtered_data, aes(x = relationship, y = cgpa, fill = relationship)) +
  geom_boxplot() +
  labs(title = "CGPA by Relationship Status",
       x = "Relationship", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")

```

```{r, echo=FALSE, warning=FALSE}
# ============================================================================
# Relationship Between Demographics and CGPA Group (CGPA Categorical on X-axis)
# ============================================================================
# This section visualizes how demographic variables vary across CGPA quartile groups (Low–Excellent).
# CGPA is discretized into 4 ordered groups.
# We examine:
# - Boxplots for continuous demographics (age, income_log)
# - Barplots for binary/categorical demographics (gender, living_with, english,
#   health_issues, disability, relationship)
# This allows us to explore how student background is associated with different CGPA levels.


# 1. Filter CGPA in desired range and create CGPA group
filtered_data <- data %>%
  filter(!is.na(cgpa), cgpa >= 1.5, cgpa <= 4) %>%
  mutate(cgpa_group = ntile(cgpa, 4)) %>%
  mutate(cgpa_group = factor(cgpa_group,
                             levels = 1:4,
                             labels = c("Low", "Average", "Good", "Excellent")))

# 2. Define demographic variables
numeric_vars <- c("age", "income_log")  # quantitative
binary_vars <- c("gender", "living_with", "english", "health_issues", "disability", "relationship")  # categorical/binary

# 3. Boxplots for numeric demographic variables
plot_list_num <- lapply(numeric_vars, function(var) {
  ggplot(filtered_data, aes_string(x = "cgpa_group", y = var, fill = "cgpa_group")) +
    geom_boxplot() +
    labs(title = paste("Distribution of", var, "by CGPA Group"),
         x = "CGPA Group", y = var) +
    theme_minimal() +
    theme(legend.position = "none")
})

# 4. Barplots for binary/categorical demographic variables (side-by-side layout)
plot_list_bin <- lapply(binary_vars, function(var) {
  ggplot(filtered_data, aes_string(x = var, fill = "cgpa_group")) +
    geom_bar(position = "dodge") +
    labs(title = paste("Distribution of", var, "by CGPA Group"),
         x = var, y = "Count") +
    theme_minimal()
})

# 5. Display the plots
for (p in plot_list_num) { print(p) }
for (p in plot_list_bin) { print(p) }

```

## Insights and references from previous research:

Across the six studies, demographic variables were consistently found to have only limited predictive power for academic outcomes, especially when stronger signals such as prior grades, study-habit measures, or behavioural clickstreams were available.
- Falat & Piscová (2022) reported that sex and age lost significance once learning-style and motivation features were included.
- Yuan (2024) identified a few demographic fields—mother’s education and family income—as useful, but still ranked below early academic scores.
- Pawitra & Hung (2024) showed that adding first-year GPA to demographics improved graduation-risk models far more than demographics alone, with first-generation and rural status acting mainly as risk modifiers.
- Asif et al. (2017) found gender and other demographic fields to contribute negligibly once early-course grades were present.
- Finally, Liu et al. (2023) achieved 90 % accuracy with click-stream data alone, noting no benefit from adding age or gender.
Taken together, these findings suggest that while certain demographic characteristics (e.g., parental education, income, first-generation status) can offer modest incremental value, final GPA is predicted far more accurately by behavioural, motivational, or early academic factors; demographics alone are insufficient for high-quality GPA prediction.

previous research:

-   Falat L. & Piscová T. (2022) – *Predicting GPA of University Students with Supervised Regression Machine-Learning Models*

1.  **Title / Authors / Year**\
    Predicting GPA of University Students with Supervised Regression Machine-Learning Models – Falat L.
    & Piscová T.

    (2022) 

2.  **Paper link**\
    <https://www.mdpi.com/2076-3417/12/17/8403>

3.  **Data link**\
    Not available (institutional survey + verified GPAs)

4.  **Sample & how collected**\
    530 Slovak undergraduates (age 18-26); 34 psycho-socio-demographic items via anonymous questionnaire; GPAs cross-checked with faculty records.

5.  **Research question**\
    Which blend of demographic, psychological and study-habit factors most accurately predicts GPA?

6.  **Models & top result**\
    Linear Regression, Decision Tree, 8 Random-Forest variants → **Random Forest MAPE ≈ 11.1 %** (5-fold CV).

7.  **Conclusions / feature insights**\
    **Demographic impact:** Sex and age added only marginal lift; self-study style and intrinsic motivation dominated.

8.  **Feature quote**\
    *“We also found that between grammar-school and vocational-school students, and between men and women, there was no statistically significant difference between grades of individual subjects except for Informatics 2.”* :contentReference[oaicite:0]{index="0"}

------------------------------------------------------------------------

-   Pawitra M. & Hung H-C. (2024) – *Machine-Learning Analysis on Students’ Demographic and Performance to Predict On-Time Graduation*

1.  **Title / Authors / Year**\
    Machine-Learning Analysis on Students’ Demographic and Performance to Predict On-Time Graduation – Pawitra M.
    & Hung H-C.

    (2024) 

2.  **Paper link**\
    <https://www.researchgate.net/publication/379037638_Machine_Learning_Analysis_on_Students_Demographic_and_Performance_to_Predict_On-Time_Graduation_A_Case_Study_in_Indonesia>

3.  **Data link**\
    Not available (institutional MIS export)

4.  **Sample & how collected**\
    ≈ 4 200 Indonesian engineering students; gender, home region, socio-economic tier + semester GPAs from mandatory registration records.

5.  **Research question**\
    Identify students at risk of delayed graduation using demographic factors.

6.  **Models & top result**\
    Random Forest, XGBoost, dynamic K-Means alert → **XGBoost pilot AUC ≈ 0.82; accuracy ≈ 75 %**.

7.  **Conclusions / feature insights**\
    **Demographic impact:** Adding Year-1 GPA to demographics raised AUC by ≈ 5 %; first-generation and rural students faced \~1.8× higher risk of delay.

8.  **Feature quote**\
    *“We anticipate identifying demographic variables, such as gender, domicile status, and enrollment type, as significant contributors to on-time graduation likelihood through K-Means clustering.”* :contentReference[oaicite:2]{index="2"}

------------------------------------------------------------------------

-   Liu Y. et al. (2023) – *Predicting Course Pass/Fail with LSTM on Clickstreams*

1.  **Title / Authors / Year**\
    Predicting Course Pass/Fail with LSTM on Clickstreams – Liu Y.
    et al. (2023)

2.  **Paper link**\
    <https://www.mdpi.com/2227-7102/13/1/17>

3.  **Data link**\
    <https://analyse.kmi.open.ac.uk/open_dataset>

4.  **Sample & how collected**\
    5 341 Open-University UK students; 213 k LMS click events per learner (no personal identifiers).

5.  **Research question**\
    Can raw clickstream behaviour alone predict pass/fail status?

6.  **Models & top result**\
    LSTM, 1D-CNN, RF, GBDT, LR; SMOTE; 10-fold CV → **LSTM accuracy ≈ 90 %**.

7.  **Conclusions / feature insights**\
    **Demographic impact:** Model achieved 90 % accuracy without demographic inputs; adding age, gender or education level gave no significant gain.

8. **Feature insight**  
    Demographic attributes (age, gender, education level) added only a negligible improvement—well under one percentage point—to the model’s accuracy; predictive power came almost entirely from the click-stream behaviour features.




## Model implementation and evaluation:

| \# | Dataset variant | Target | Best model | Test-set metrics | Top predictors (RF importance) | Comment |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 1 | Full Data | `cgpa` | Linear Reg. (= RF) | R² 0.04 · RMSE 0.77 · MAE 0.50 | `english_Basic` 0.038 · `english_Intermediate` 0.029 · `relationship_Married` 0.018 | Practically no signal – predicts mean GPA |
| 2 | Full Data | `cgpa_scaled` | Linear Reg. (= RF) | R² 0.04 · RMSE 19.29 · MAE 12.46 | same order (scaled) | Scaling changes units only |
| 3 | Cleaned Data | `cgpa` | Linear Reg. (= RF) | R² 0.01 · RMSE 0.47 · MAE 0.39 | `english_Basic` 0.015 · `living_with_Family` 0.008 · `relationship_Married` 0.007 | Cleaning trims variance; still no predictive value |
| 4 | Cleaned Data | `cgpa_scaled` | Linear Reg. (= RF) | R² 0.01 · RMSE 11.95 · MAE 9.99 | same order | — |
| 5 | Full Data | `cgpa_class` | RF Classifier | Acc 0.325 · F1 0.324 | `english_Basic` 0.009 · `relationship_Married` 0.006 · `age` 0.004 | Only \~5 pp above random (25 %) |
| 6 | Cleaned Data | `cgpa_class` | RF Classifier | Acc 0.373 · F1 0.368 | `relationship_Married` 0.007 · `english_Basic` 0.007 · `english_Intermediate` 0.006 | Small accuracy lift after cleaning, still weak |

Modeling Summary – Demographic-Only Models

1.  **Demographics Alone Perform Poorly:**\
    All models trained solely on demographic features (e.g., age, income, relationship, English level) produced **R² ≈ 0.00** and **RMSE ≈ 0.75** on the 0–4 CGPA scale.\
    Classifier performance was similarly poor: **random-forest accuracy = 30–37 %**, barely above random.

2.  **Filtering Did Not Help:**\
    Removing outliers (extreme income or low gardes) had negligible impact.\
    Regression R² remained close to zero, and classification accuracy improved by only **+2–3 percentage points**.

3.  **Feature Importance Is Minimal:**\
    No demographic variable explained more than **1–4 %** of the variance.\
    Indicators like English proficiency, marital status, income, and age had near-zero contribution; regression coefficients were mostly non-significant.

4.  **Model Type Is Irrelevant:**\
    No meaningful gain from using Random Forest over Linear Regression.\
    With weak predictors, even complex models cannot outperform simple ones.

5.  **Conclusion:**\
    **Demographics alone are insufficient** for GPA prediction.\
    To achieve meaningful accuracy, models must include academic or behavioural features (e.g., prior grades, learning behaviour, LMS logs).

```{r, echo=FALSE, warning=FALSE}
# Global list to collect feature importances
all_importances <- list()

run_model_analysis <- function(data, target_col, task_type = c("regression", "classification"), dataset_name = "") {
  task_type <- match.arg(task_type)

  set.seed(123)
  data_split <- initial_split(data, prop = 0.8, strata = !!sym(target_col))
  train_data <- training(data_split)
  test_data <- testing(data_split)

  set.seed(123)
  cv_folds <- vfold_cv(train_data, v = 5, strata = !!sym(target_col))

  predictors <- c("age ","gender", "income_log","living_with",
                  "english", "health_issues", "disability","relationship")

  cat_vars <- c( "gender", "english","health_issues", "disability","relationship","living_with")
                

  model_recipe <- recipe(as.formula(paste(target_col, "~", paste(predictors, collapse = "+"))), data = train_data) %>%
    step_mutate_at(cat_vars, fn = as.factor) %>%
    step_nzv(all_predictors()) %>%
    step_corr(all_numeric_predictors(), threshold = 0.9) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors())

  if (task_type == "regression") {
    models <- list(
      "Linear Regression" = linear_reg() %>% set_engine("lm"),
      "Random Forest Regression" = rand_forest(mode = "regression", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  } else {
    models <- list(
      "Random Forest Classifier" = rand_forest(mode = "classification", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  }

  for (model_name in names(models)) {
    cat("\n\U0001F539 Running:", model_name, "on target:", target_col, "with dataset:", dataset_name, "\n")

    wflow <- workflow() %>%
      add_model(models[[model_name]]) %>%
      add_recipe(model_recipe)

    # Cross-validation metrics manually
    cv_preds <- vector("list", length = 5)
    for (i in seq_along(cv_folds$splits)) {
      split <- cv_folds$splits[[i]]
      train_fold <- analysis(split)
      test_fold <- assessment(split)
      model_fit <- fit(wflow, data = train_fold)
      preds <- predict(model_fit, new_data = test_fold, type = ifelse(task_type == "classification", "class", "numeric")) %>%
        bind_cols(test_fold %>% select(all_of(target_col)))
      cv_preds[[i]] <- preds
    }
    all_cv_preds <- bind_rows(cv_preds)

    cat("\n\U0001F4C8 Cross-validation performance (manual):\n")
    if (task_type == "regression") {
      r1 <- rmse(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      cat(sprintf("RMSE: %.4f | R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, r3$.estimate))
    } else {
      a <- accuracy(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
    }

    # Final model
    final_fit <- fit(wflow, data = train_data)
    test_pred <- predict(final_fit, new_data = test_data, type = ifelse(task_type == "classification", "class", "numeric")) %>%
      bind_cols(test_data %>% select(all_of(target_col)))

    cat("\n\U0001F4CA Test set performance:\n")
    if (task_type == "regression") {
      r1 <- rmse(test_pred, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(test_pred, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(test_pred, truth = !!sym(target_col), estimate = .pred)
      adj_r2 <- 1 - (1 - r2$.estimate) * ((nrow(test_data) - 1) / (nrow(test_data) - length(predictors) - 1))
      cat(sprintf("RMSE: %.4f | R²: %.4f | Adjusted R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, adj_r2, r3$.estimate))
    } else {
      a <- accuracy(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
      cat("\n📉 Confusion Matrix:\n")
      print(conf_mat(test_pred, truth = !!sym(target_col), estimate = .pred_class))
    }
    
    

    # Feature importance or coefficients
    model_fit <- extract_fit_parsnip(final_fit)
    if (inherits(model_fit$fit, "lm") || inherits(model_fit$fit, "glm")) {
      coef_df <- broom::tidy(model_fit$fit) %>%
        mutate(model = model_name, target = target_col, dataset = dataset_name)
      print(coef_df)
      all_importances[[length(all_importances) + 1]] <- coef_df
    } else if (!is.null(model_fit$fit$variable.importance)) {
      imp_tbl <- enframe(model_fit$fit$variable.importance, name = "feature", value = "importance") %>%
        arrange(desc(importance)) %>%
        mutate(model = model_name, target = target_col, dataset = dataset_name)
      print(imp_tbl)
      all_importances[[length(all_importances) + 1]] <- imp_tbl
    } else {
      cat("\n⚠️ Feature importance not available for model:", model_name, "\n")
    }
  }
}

# Dataset splitting to avoid cgpa_class leakage
all_data_variants <- list(
  "Full Data - Regr" = data %>% select(-cgpa_class),
  "Cleaned Data - Regr" = data_clean %>% select(-cgpa_class),
  "Full Data - Class" = data,
  "Cleaned Data - Class" = data_clean
)

targets <- list(
  list(col = "cgpa", type = "regression", dataset = "Full Data - Regr"),
  list(col = "cgpa_scaled", type = "regression", dataset = "Full Data - Regr"),
  list(col = "cgpa", type = "regression", dataset = "Cleaned Data - Regr"),
  list(col = "cgpa_scaled", type = "regression", dataset = "Cleaned Data - Regr"),
  list(col = "cgpa_class", type = "classification", dataset = "Full Data - Class"),
  list(col = "cgpa_class", type = "classification", dataset = "Cleaned Data - Class")
)

for (target in targets) {
  dname <- target$dataset
  cat("\n=============================\n")
  cat("Dataset:", dname, "| Target:", target$col, "| Type:", target$type, "\n")
  cat("=============================\n")
  run_model_analysis(all_data_variants[[dname]], target$col, task_type = target$type, dataset_name = dname)
}

```

## Results analysis and conclusions

Descriptive analysis shows a largely homogeneous cohort: most students are 20–22 years old, report low-to-moderate household income, and rate their English as “Intermediate”.
Health issues and disabilities are rare, and the gender split is only slightly male-leaning, hinting at limited predictive power.
Correlation checks confirm this intuition—every demographic variable correlates negligibly with CGPA (\|r\| ≤ 0.10) and inter-demographic correlations never exceed \|r\| = 0.22.
**Model testing confirms these findings:** both linear and Random-Forest regressors explain virtually no variance (R² ≈ 0), and classifiers achieve only 30–37 % accuracy, a marginal improvement over chance.
Removing extreme-income or low grades changes neither fit nor accuracy by more than three percentage points.
Feature-importance scores never exceed 4 %.

| Variant   | Task              | Best Model & Metrics                      |
|-----------|-------------------|-------------------------------------------|
| Full data | CGPA (regression) | Linear Reg. ≈ RF, **R² 0.04 / RMSE 0.77** |
| Cleaned   | CGPA (regression) | Linear Reg. ≈ RF, **R² 0.01 / RMSE 0.47** |
| Full data | CGPA class (A–D)  | RF Classifier, **Acc 0.325 / F1 0.324**   |
| Cleaned   | CGPA class (A–D)  | RF Classifier, **Acc 0.373 / F1 0.368**   |

These outcomes echo six external studies (Falat & Piscová 2022; Yuan 2024; Pawitra & Hung 2024; Asif et al. 2017; Beaulac & Rosenthal 2019; Liu et al. 2023): once early grades, study habits, or behavioural logs are present, demographics add—at best—modest incremental lift and often none at all.

**Conclusion:** demographic characteristics alone are insufficient for high-quality GPA prediction.
Meaningful forecasting demands richer academic or behavioural features such as prior-semester grades, study-time logs, attendance data, or LMS clickstreams.

**Recommended next steps:**\
1.
**Leverage demographics for equity auditing, not prediction** – use age, gender, income tier, and first-generation status to monitor model fairness and identify underserved groups rather than as primary predictors of GPA.\
2.
**Use demographic features to flag disadvantaged groups after deploying a strong academic-performance model** – once at-risk students are identified by reliable grade-based predictors, apply demographic slices to tailor support and resources to specific population segments.

# q3 - Can the final GPA be predicted based on learning behavior and study habits?

## Analysis of predictive features:

-   **attendance** – Most students reported near-perfect attendance, with a strong concentration at 100%.

-   **study_hours** – Study hours are concentrated between 2–4 hours/day, with a long tail toward higher values.

-   **study_freq** – The majority study 2–3 times per day; very few report higher frequencies.

-   **learning_mode** – Most students learn offline; online mode is less common.

-   **teacher_consult** – Roughly equal split between students who do and don’t consult teachers.

-   **co_curricular** – Slightly more students report not participating in co-curricular activities than those who do.

-   **Study_Attendance_Interaction** - The histogram of Study_Attendance_Interaction is right-skewed, with most students scoring between 1 and 5.
    This variable multiplies study hours by attendance rate, reflecting practical study commitment.

-   **Study_Efficiency** - The Study_Efficiency histogram shows a sharp peak around 2, with a long tail of higher values.
    This suggests most students average \~2 hours per session, with a few highly efficient outliers.

-   **TotalStudyEffort** - The TotalStudyEffort score also displays right-skewed distribution, centering around 3–5.
    This combined index incorporates attendance-adjusted study time and engagement indicators (teacher consultations and co-curricular activity).

```{r, echo=FALSE, warning=FALSE}
# ----------------------------------------------------------------------------
# Distribution of numerical variables: histograms and summary stats
# ----------------------------------------------------------------------------
# This section generates histograms and descriptive statistics for the key
# quantitative learning behavior variables:
# - study_hours: number of study hours per day
# - study_freq: frequency of study sessions
# - attendance: self-reported class attendance percentage

quant_vars <- c("study_hours", "study_freq", "attendance","Study_Attendance_Interaction","Study_Efficiency","TotalStudyEffort")

for (var in quant_vars) {
  plt <- ggplot(data, aes_string(x = var)) +
    geom_histogram(fill = "skyblue", color = "black", bins = 15) +
    labs(title = paste("Distribution of", var), x = var, y = "Count") +
    theme_minimal()
  
  print(plt)
}

# ----------------------------------------------------------------------------
# Distribution of categorical variables: frequency tables and bar plots
# ----------------------------------------------------------------------------
# This section analyzes the categorical learning behavior variables:
# - learning_mode: preferred learning method (solo / group / other)
# - teacher_consult: frequency of seeking help from instructors
# - co_curricular: participation in co-curricular activities
#
# For each variable, the code prints a frequency table and bar plot to
# explore the proportion of students choosing each category.

cat_vars <- c("learning_mode", "teacher_consult", "co_curricular")

for (var in cat_vars) {
  plt <- ggplot(data, aes_string(x = var)) +
    geom_bar(fill = "lightgreen", color = "black") +
    labs(title = paste("Bar Plot of", var), x = var, y = "Count") +
    theme_minimal()
  
  print(plt)
}
```

1.  **Clustered Distributions: Limited Predictive Power**

-   **65%** of students report attendance ≥ 90%.
-   **80%** study between **1–4 hours/day**.
-   **95%** report study frequency of **1–3 times/day**.
-   **70%** have CGPA in the narrow range **2.8–3.8**.

**Conclusion:**\
When most values cluster in tight intervals, variables may lack variance and offer limited predictive signal in modeling.

2.  **Potential Concerns Around Self-Reported Attendance**

-   **69** students report **100% attendance** but have **CGPA \< 3.0**.
-   Among all with perfect attendance, CGPA ranges **2.0–4.0** with **SD = 0.45**.

**Conclusion:**\
The wide variation in CGPA among students reporting perfect attendance raises questions about the accuracy of self-reported attendance data, or suggests that attendance alone may not fully reflect academic engagement.

3.  **Derived Features May Add Limited Value**

-   `TotalStudyEffort` is highly correlated with both `study_hours` (r = 0.87) and `Study_Attendance_Interaction` (r = 0.92).
-   `Study_Attendance_Interaction` strongly overlaps with `study_hours` (r = 0.94) and `attendance` (r = 0.40).
-   `Study_Efficiency` reflects normalized hours (r = 0.62 with `study_hours`, r = -0.36 with `study_freq`).

**Conclusion:**\
The derived features are not independent and closely mirror the original variables.
While they may offer interpretability, their predictive value is likely redundant unless used as replacements.

```{r, echo=FALSE, warning=FALSE}

# ----------------------------------------------------------------------------
# Claim: Core behavioral variables may lack variance (not informative)
# ----------------------------------------------------------------------------

# Study hours: Proportion of students studying between 1–4 hours
filtered_data %>%
  summarise(Proportion_of_students_studying_1_to_4_hours = mean(study_hours >= 1 & study_hours <= 4))

# Attendance: Proportion of students with attendance ≥ 90%
filtered_data %>%
  summarise(Proportion_of_students_with_attendance_bigger_90_precente = mean(attendance >= 90))

# Study frequency: Proportion of students studying 1–3 times a day
filtered_data %>%
  summarise(Proportion_of_students_studying_1_to_3_times_day = mean(study_freq >= 1 & study_freq <= 3))

# CGPA: Proportion of students with CGPA between 2.8–3.8
filtered_data %>%
  summarise(Proportion_of_students_with_CGPA_between_2.8_to_3.8 = mean(cgpa >= 2.8 & cgpa <= 3.8))

# ----------------------------------------------------------------------------
# Claim: Some self-reported values may be inflated
# ----------------------------------------------------------------------------

# 100% Attendance + CGPA < 3.0
filtered_data %>%
  filter(attendance == 100 & cgpa < 3) %>%
  summarise(count = n())

# CGPA stats among 100% attendance
filtered_data %>%
  filter(attendance == 100) %>%
  summarise(min_cgpa_100_prec_Attendance = min(cgpa), max_cgpa_100_prec_Attendance = max(cgpa), sd_cgpa_100_prec_Attendance = sd(cgpa))

# ============================================================================
# Study Behavior Variables - Correlation Matrix
# ============================================================================

# Subset to relevant study variables
study_vars <- c("study_freq", "study_hours", "attendance", 
                "Study_Attendance_Interaction", "Study_Efficiency", "TotalStudyEffort")

# Clean data
study_corr_data <- filtered_data[, study_vars] %>% na.omit()

# Compute correlation matrix
corr_matrix <- cor(study_corr_data, use = "complete.obs")

# ============ Plot Settings ============
par(mfrow = c(1, 1), mar = c(8, 8, 4, 6))  # wider plot area

# Color scale: blue (neg) to red (pos)
colors <- colorRampPalette(c("#B2182B", "#D6604D", "#F4A582", "#FDDBC7", 
                             "#F7F7F7", "#D1E5F0", "#92C5DE", "#4393C3", "#2166AC"))(100)

# Draw the matrix
image(1:ncol(corr_matrix), 1:nrow(corr_matrix), 
      t(corr_matrix[nrow(corr_matrix):1, ]),
      col = colors, axes = FALSE, xlab = "", ylab = "",
      main = "Study Habits Correlation Matrix", cex.main = 1.4,
      zlim = c(-1, 1))

# Add axis labels
axis(1, at = 1:ncol(corr_matrix), labels = colnames(corr_matrix), 
     las = 2, cex.axis = 1)
axis(2, at = 1:nrow(corr_matrix), 
     labels = rev(rownames(corr_matrix)), las = 2, cex.axis = 1)

# Add correlation values in white (or black for contrast)
for(i in 1:nrow(corr_matrix)) {
  for(j in 1:ncol(corr_matrix)) {
    r <- corr_matrix[i, j]
    col_text <- ifelse(abs(r) > 0.5, "white", "black")
    text(j, nrow(corr_matrix) - i + 1, sprintf("%.2f", r), 
         col = col_text, cex = 1, font = 2)
  }
}

# Legend
legend("right", title = "Correlation", 
       legend = c("1.0", "0.5", "0.0", "-0.5", "-1.0"),
       fill = c(colors[100], colors[75], colors[50], colors[25], colors[1]),
       cex = 1, inset = c(-0.25, 0), xpd = TRUE)

# Grid
abline(h = (0:nrow(corr_matrix)) + 0.5, col = "white", lwd = 1)
abline(v = (0:ncol(corr_matrix)) + 0.5, col = "white", lwd = 1)

# Reset margins
par(mar = c(5, 4, 4, 2))
```

## Correlation analysis between features and with the target:

-   **`study_hours` vs. `study_freq`:**\
    Moderate positive correlation (**r = 0.374**).
    Students who study more frequently tend to study more hours per day overall.

-   **`attendance` vs. Study Patterns:**\
    Weak correlations with both `study_freq` (**r = 0.06**) and `study_hours` (**r = 0.09**), suggesting limited alignment between reported attendance and self-reported study effort.

-   **`teacher_consult`, `co_curricular`, `learning_mode`:**\
    Very weak correlations (all \|r\| \< 0.06) with other study variables, indicating minimal overlap with core study behaviors.

**Conclusion:**\
Only the relationship between `study_hours` and `study_freq` shows meaningful linear correlation.\
Other variables exhibit weak linear associations, which may limit their predictive utility in linear models, but non-linear models may still extract signal through interaction effects or threshold patterns.

```{r, echo=FALSE, warning=FALSE}
# ============================================================================
# Study Habits Variables Correlation Matrix
# ============================================================================

# ============================================================================
# 1. Load Data and Define Variables
# ============================================================================

# Read the data if not already loaded
if(!exists("data") || is.null(data)) {
  # Try to load from common data object names
  if(exists("train_data")) {
    data <- train_data
  } else if(exists("df")) {
    data <- df
  } else if(exists("dataset")) {
    data <- dataset
  } else {
    #cat("❌ Please load your data first into a variable called 'data'\n")
    #cat("Example: data <- read.csv('data.csv')\n")
    stop("No data found")
  }
}



# Study habits variables based on your data columns
study_vars <- c("study_freq",         # Study frequency per day
                "study_hours",        # Study hours per day
                "attendance",         # Attendance percentage
                "learning_mode",      # Online/Offline learning
                "teacher_consult",    # Teacher consultation
                "co_curricular")      # Co-curricular activities

# Variable descriptions
var_descriptions <- c("Study frequency per day", 
                     "Study hours per day", 
                     "Attendance rate", 
                     "Learning mode (Online/Offline)", 
                     "Teacher consultation", 
                     "Co-curricular activities")

available_vars <- character()
missing_vars <- character()

for(i in 1:length(study_vars)) {
  if(study_vars[i] %in% names(data)) {
    #cat(sprintf("✅ %s (%s)\n", var_descriptions[i], study_vars[i]))
    available_vars <- c(available_vars, study_vars[i])
  } else {
    #cat(sprintf("❌ %s (%s) - NOT FOUND\n", var_descriptions[i], study_vars[i]))
    missing_vars <- c(missing_vars, study_vars[i])
  }
}

# ============================================================================
# 2. Prepare Data for Analysis
# ============================================================================

if(length(available_vars) >= 2) {
  
  # Select only the available study habits variables
  study_data <- data[, available_vars, drop = FALSE]
  
  # Check data types and convert if necessary
  for(var in available_vars) {
    original_type <- class(study_data[[var]])[1]
    
    if(is.character(study_data[[var]])) {
      study_data[[var]] <- as.factor(study_data[[var]])
      #cat(sprintf("- %s: converted from character to factor\n", var))
    }
    
    if(is.factor(study_data[[var]])) {
      levels_count <- length(levels(study_data[[var]]))
      if(levels_count == 2) {
        # Binary factor: convert to 0/1
        study_data[[var]] <- as.numeric(study_data[[var]]) - 1
        #cat(sprintf("- %s: binary factor converted to 0/1\n", var))
      } else {
        # Multi-level factor: convert to numeric codes
        study_data[[var]] <- as.numeric(study_data[[var]])
        #cat(sprintf("- %s: factor with %d levels converted to numeric\n", var, levels_count))
      }
    } else if(is.numeric(study_data[[var]])) {
      #cat(sprintf("- %s: already numeric (%s)\n", var, original_type))
    }
  }
  
  # Remove rows with missing values
  complete_data <- study_data[complete.cases(study_data), ]
  missing_rows <- nrow(study_data) - nrow(complete_data)
  

  
  # ============================================================================
  # 3. Calculate Correlation Matrix
  # ============================================================================
  
  if(nrow(complete_data) >= 30 && ncol(complete_data) >= 2) {
    
    # Calculate Pearson correlation matrix
    corr_matrix <- cor(complete_data, use = "complete.obs")
    
    # Display the correlation matrix
    cat("\n", paste(rep("=", 70), collapse = ""))
    cat("\nPEARSON CORRELATION MATRIX - STUDY HABITS")
    cat("\n", paste(rep("=", 70), collapse = ""))
    cat("\n")
    print(round(corr_matrix, 3))
    
    # ============================================================================
    # 4. Find Strong Correlations
    # ============================================================================
    
    cat("\n🔍 STRONG CORRELATIONS (|r| ≥ 0.3):\n")
    cat(paste(rep("-", 70), collapse = ""), "\n")
    
    strong_corr <- data.frame(
      Variable1 = character(),
      Variable2 = character(),
      Correlation = numeric(),
      Strength = character(),
      stringsAsFactors = FALSE
    )
    
    # Find correlations above threshold
    for(i in 1:(nrow(corr_matrix)-1)) {
      for(j in (i+1):ncol(corr_matrix)) {
        r_value <- corr_matrix[i,j]
        
        if(abs(r_value) >= 0.3) {
          strength <- if(abs(r_value) >= 0.7) "Very Strong" else
                     if(abs(r_value) >= 0.5) "Strong" else "Moderate"
          
          strong_corr <- rbind(strong_corr, data.frame(
            Variable1 = rownames(corr_matrix)[i],
            Variable2 = colnames(corr_matrix)[j],
            Correlation = r_value,
            Strength = strength
          ))
        }
      }
    }
    
    if(nrow(strong_corr) > 0) {
      # Sort by absolute correlation value
      strong_corr <- strong_corr[order(abs(strong_corr$Correlation), decreasing = TRUE), ]
      
      cat(sprintf("%-15s %-15s %12s %15s\n", "Variable 1", "Variable 2", "Correlation", "Strength"))
      cat(paste(rep("-", 70), collapse = ""), "\n")
      
      for(i in 1:nrow(strong_corr)) {
        cat(sprintf("%-15s %-15s %12.3f %15s\n", 
                    strong_corr$Variable1[i], 
                    strong_corr$Variable2[i], 
                    strong_corr$Correlation[i], 
                    strong_corr$Strength[i]))
      }
    } else {
      cat("No correlations found with |r| ≥ 0.3\n")
    }
    
    # ============================================================================
    # 5. Correlations with CGPA
    # ============================================================================
    
    if("cgpa" %in% names(data)) {
      cat("\n🎯 CORRELATIONS WITH CGPA:\n")
      cat(paste(rep("-", 50), collapse = ""), "\n")
      
      # Prepare data including CGPA
      cgpa_analysis <- data[, c("cgpa", available_vars), drop = FALSE]
      
      # Convert categorical variables for CGPA analysis
      for(var in available_vars) {
        if(is.character(cgpa_analysis[[var]])) {
          cgpa_analysis[[var]] <- as.factor(cgpa_analysis[[var]])
        }
        if(is.factor(cgpa_analysis[[var]])) {
          if(length(levels(cgpa_analysis[[var]])) == 2) {
            cgpa_analysis[[var]] <- as.numeric(cgpa_analysis[[var]]) - 1
          } else {
            cgpa_analysis[[var]] <- as.numeric(cgpa_analysis[[var]])
          }
        }
      }
      
      # Remove missing values
      cgpa_analysis <- cgpa_analysis[complete.cases(cgpa_analysis), ]
      
      # Calculate correlations with CGPA
      cgpa_corr <- cor(cgpa_analysis[, available_vars], cgpa_analysis$cgpa)
      cgpa_corr <- sort(cgpa_corr, decreasing = TRUE)
      
      cat(sprintf("%-20s %12s %15s\n", "Study Habit", "Correlation", "Interpretation"))
      cat(paste(rep("-", 50), collapse = ""), "\n")
      
      for(i in 1:length(cgpa_corr)) {
        var_name <- names(cgpa_corr)[i]
        r_val <- cgpa_corr[i]
        
        interpretation <- if(abs(r_val) >= 0.5) "Strong effect" else
                         if(abs(r_val) >= 0.3) "Moderate effect" else
                         if(abs(r_val) >= 0.1) "Weak effect" else "Minimal effect"
        
        cat(sprintf("%-20s %12.3f %15s\n", var_name, r_val, interpretation))
      }
      
      cat(sprintf("\nCGPA analysis sample size: %d students\n", nrow(cgpa_analysis)))
    }
    
    # ============================================================================
    # 6. Visualization
    # ============================================================================
    
    cat("\n📊 Creating correlation heatmap...\n")
    
    # Set up the plot
    par(mfrow = c(1, 1), mar = c(10, 10, 4, 8))
    
    # Color palette - properly ordered for -1 to +1 scale
    colors <- colorRampPalette(c("#B2182B", "#D6604D", "#F4A582", "#FDDBC7", 
                               "#F7F7F7", "#D1E5F0", "#92C5DE", "#4393C3", "#2166AC"))(100)
    
    # Create the heatmap with proper color scaling
    image(1:ncol(corr_matrix), 1:nrow(corr_matrix), 
          t(corr_matrix[nrow(corr_matrix):1, ]),
          col = colors,
          axes = FALSE,
          xlab = "", ylab = "",
          main = "Study Habits Correlation Matrix",
          cex.main = 1.2,
          zlim = c(-1, 1))  # Ensure proper color scaling from -1 to 1
    
    # Add variable labels
    axis(1, at = 1:ncol(corr_matrix), labels = colnames(corr_matrix), 
         las = 2, cex.axis = 0.9)
    axis(2, at = 1:nrow(corr_matrix), 
         labels = rev(rownames(corr_matrix)), las = 2, cex.axis = 0.9)
    
    # Add correlation values as text
    for(i in 1:nrow(corr_matrix)) {
      for(j in 1:ncol(corr_matrix)) {
        text_color <- ifelse(abs(corr_matrix[i,j]) > 0.6, "white", "black")
        text(j, nrow(corr_matrix) - i + 1, 
             sprintf("%.2f", corr_matrix[i,j]), 
             col = text_color, cex = 0.8, font = 2)
      }
    }
    
    # Add legend with correct color mapping
    legend("right", title = "Correlation", 
           legend = c("1.0", "0.5", "0.0", "-0.5", "-1.0"),
           fill = c(colors[100], colors[75], colors[50], colors[25], colors[1]),
           cex = 0.8, inset = c(-0.3, 0), xpd = TRUE)
    
    # Add grid
    abline(h = (0:nrow(corr_matrix)) + 0.5, col = "white", lwd = 1)
    abline(v = (0:ncol(corr_matrix)) + 0.5, col = "white", lwd = 1)
    
    # Reset margins
    par(mar = c(5, 4, 4, 2))
    
    # ============================================================================
    # 7. Summary and Insights
    # ============================================================================
    
    cat("\n", paste(rep("=", 70), collapse = ""))
    cat("\nSUMMARY AND INSIGHTS")
    cat("\n", paste(rep("=", 70), collapse = ""))
    
    cat(sprintf("\n📈 ANALYSIS OVERVIEW:\n"))
    cat(sprintf("• Variables analyzed: %d out of %d requested\n", 
                length(available_vars), length(study_vars)))
    cat(sprintf("• Sample size: %d students\n", nrow(complete_data)))
    cat(sprintf("• Strong correlations found: %d\n", nrow(strong_corr)))
    
    if(nrow(strong_corr) > 0) {
      strongest <- strong_corr[1, ]
      cat(sprintf("• Strongest relationship: %s ↔ %s (r = %.3f)\n", 
                  strongest$Variable1, strongest$Variable2, strongest$Correlation))
    }
    
    cat(sprintf("\n🎓 KEY FINDINGS:\n"))
    
    # Study intensity analysis
    if(all(c("study_freq", "study_hours") %in% available_vars)) {
      freq_hours_r <- corr_matrix["study_freq", "study_hours"]
      cat(sprintf("• Study frequency vs hours: r = %.3f ", freq_hours_r))
      if(freq_hours_r > 0.3) {
        cat("(More frequent study sessions = more total hours)\n")
      } else if(freq_hours_r < -0.3) {
        cat("(Frequent short sessions vs fewer long sessions)\n")
      } else {
        cat("(No clear pattern between frequency and total hours)\n")
      }
    }
    
    # Attendance patterns
    if("attendance" %in% available_vars) {
      if("study_hours" %in% available_vars) {
        att_study_r <- corr_matrix["attendance", "study_hours"]
        cat(sprintf("• Attendance vs study hours: r = %.3f ", att_study_r))
        if(att_study_r > 0.2) {
          cat("(Better attendance linked to more study)\n")
        } else {
          cat("(Attendance and study hours not strongly linked)\n")
        }
      }
    }
    
    # Learning mode effects
    if("learning_mode" %in% available_vars && "attendance" %in% available_vars) {
      mode_att_r <- corr_matrix["learning_mode", "attendance"]
      cat(sprintf("• Learning mode vs attendance: r = %.3f\n", mode_att_r))
    }
    
    # Best predictors of academic success
    if("cgpa" %in% names(data)) {
      best_predictor <- names(cgpa_corr)[1]
      best_r <- cgpa_corr[1]
      cat(sprintf("• Best CGPA predictor: %s (r = %.3f)\n", best_predictor, best_r))
      
      if(best_r > 0.3) {
        cat("  → Strong positive relationship with academic performance\n")
      } else if(best_r > 0.1) {
        cat("  → Moderate positive relationship with academic performance\n")
      } else {
        cat("  → Weak relationship with academic performance\n")
      }
    }
    
    cat(sprintf("\n📝 NOTES:\n"))
    cat("• Correlation does not imply causation\n")
    cat("• Values range from -1 (perfect negative) to +1 (perfect positive)\n")
    cat("• |r| ≥ 0.3 considered practically significant\n")
    
    if(length(missing_vars) > 0) {
      cat(sprintf("• Missing variables: %s\n", paste(missing_vars, collapse = ", ")))
    }
    
  } else {
    cat("❌ INSUFFICIENT DATA for correlation analysis\n")
    cat(sprintf("Required: ≥30 observations and ≥2 variables\n"))
    cat(sprintf("Available: %d observations, %d variables\n", 
                nrow(complete_data), ncol(complete_data)))
  }
  
} else {
  cat("❌ INSUFFICIENT VARIABLES found\n")
  cat("Need at least 2 study habits variables for correlation analysis\n")
  if(length(missing_vars) > 0) {
    cat("\nMissing variables that need to be created or renamed:\n")
    for(var in missing_vars) {
      cat(sprintf("- %s\n", var))
    }
  }
}

#cat("\n", paste(rep("=", 50), collapse = ""))
#cat("\nANALYSIS COMPLETE")
#cat("\n", paste(rep("=", 50), collapse = ""))

# Check if data exists
if(!exists("data") || is.null(data)) {
  if(exists("train_data")) {
    data <- train_data
  } else if(exists("df")) {
    data <- df
  } else {
    cat("❌ Please load your data first\n")
    stop("No data found")
  }
}

# Check if required variables exist
#if(!all(c("study_hours", "study_freq") %in% names(data))) {
#  stop("Missing required variables: study_hours and/or study_freq")
#}

# Prepare the data
#plot_data <- data[, c("study_hours", "study_freq")]
#plot_data <- plot_data[complete.cases(plot_data), ]

# Calculate correlation
#correlation <- cor(plot_data$study_hours, plot_data$study_freq)

# Create the scatter plot
#plot(plot_data$study_freq, plot_data$study_hours,
#     main = "Study Hours vs Study Frequency",
#     sub = paste("Correlation: r =", round(correlation, 3)),
#     xlab = "Study Frequency (times per day)",
#     ylab = "Study Hours (hours per day)",
#     pch = 19,
#     col = "steelblue",
#     cex = 1.2,
#     cex.main = 1.3,
#     cex.sub = 1.1,
#     cex.lab = 1.1)

# Add regression line
#lm_fit <- lm(study_hours ~ study_freq, data = plot_data)
#abline(lm_fit, col = "red", lwd = 2)

# Add grid
#grid(col = "lightgray", lty=1,lwd=0.5)
```

## Relationship analysis with the target variable:

Interpretation of Learning Behavior Variables by CGPA Group

-   **Study Hours → CGPA:**\
    Students who study more hours per day tend to have higher CGPA.\
    The median CGPA increases consistently from “Low” to “Very High” study hour groups, indicating that time spent studying correlates with better academic outcomes.

-   **Study Frequency → CGPA:**\
    A similar positive trend is observed with study frequency.\
    Students in the “Very High” group (studying most frequently) show higher CGPA than those in lower groups.\
    However, distributions overlap, and frequency alone may not fully capture study quality.

-   **Attendance → CGPA:**\
    While higher attendance aligns weakly with higher CGPA, the relationship is noisy.\
    Many students across all CGPA groups report full attendance, limiting the variable’s ability to distinguish performance.

-   **Learning Mode → CGPA:**\
    Offline learners are more common in higher CGPA groups, suggesting possible benefit.\
    Still, since this variable captures only mode (not quality of learning), interpretation is limited.

-   **Teacher Consultation → CGPA:**\
    Students who consulted with teachers appear across all CGPA groups, with no strong directional trend.

-   **Co-Curricular Participation → CGPA:**\
    Slightly more students in higher CGPA groups reported co-curricular participation, but the difference is small.

**Overall:** - Learning behavior variables like study hours and frequency show positive trends with CGPA.
- Other variables (e.g., attendance) are either too coarse or may be affected by self-report bias.
- The lack of strong separation across CGPA groups suggests limited predictive power, especially for non-linear or noisy variables.

```{r, echo=FALSE, warning=FALSE}

# ============================================================================
# Relationship Between Study Behavior and CGPA (Continuous on Y-axis)
# ============================================================================
# This section visualizes how CGPA (Y-axis) varies across study behavior variables.
# Each plot shows CGPA distribution across:
# - Discretized variables (study_hours, study_freq, attendance) split into 4 equal-width bins
# - Binary/categorical variables (learning_mode, teacher_consult, co_curricular)
# This helps assess how CGPA shifts across different patterns of study behavior.

# Remove NAs and discretize study_hours into 4 groups
filtered_data <- data %>%
  filter(!is.na(study_hours), !is.na(cgpa)) %>%
  mutate(study_hours_group = cut(study_hours,
                                 breaks = 4,
                                 labels = c("Low", "Medium", "High", "Very High"),
                                 include.lowest = TRUE))

# Boxplot of CGPA by study hours group
ggplot(filtered_data, aes(x = study_hours_group, y = cgpa, fill = study_hours_group)) +
  geom_boxplot() +
  labs(title = "CGPA by Study Hours Group",
       x = "Study Hours Group", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")


# Remove NAs and discretize study_freq into 4 groups
filtered_data <- data %>%
  filter(!is.na(study_freq), !is.na(cgpa)) %>%
  mutate(study_freq_group = cut(study_freq,
                                breaks = 4,
                                labels = c("Low", "Medium", "High", "Very High"),
                                include.lowest = TRUE))

# Boxplot of CGPA by study frequency group
ggplot(filtered_data, aes(x = study_freq_group, y = cgpa, fill = study_freq_group)) +
  geom_boxplot() +
  labs(title = "CGPA by Study Frequency Group",
       x = "Study Frequency Group", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")



# Remove NAs and discretize attendance into 4 groups
filtered_data <- data %>%
  filter(!is.na(attendance), !is.na(cgpa)) %>%
  mutate(attendance_group = cut(attendance,
                                breaks = 4,
                                labels = c("Low", "Medium", "High", "Very High"),
                                include.lowest = TRUE))

# Boxplot of CGPA by attendance group
ggplot(filtered_data, aes(x = attendance_group, y = cgpa, fill = attendance_group)) +
  geom_boxplot() +
  labs(title = "CGPA by Attendance Group",
       x = "Attendance Group (%)", y = "CGPA") +
  theme_minimal() +
  theme(legend.position = "none")

# ------------------ LEARNING_MODE vs CGPA ------------------

# Boxplot of CGPA by learning mode
ggplot(data, aes(x = learning_mode, y = cgpa, fill = learning_mode)) +
  geom_boxplot() +
  labs(title = "CGPA by Learning Mode", x = "Learning Mode", y = "CGPA") +
  theme_minimal()

# ------------------ TEACHER_CONSULT vs CGPA ------------------

# Boxplot of CGPA by teacher consultation attendance
ggplot(data, aes(x = teacher_consult, y = cgpa, fill = teacher_consult)) +
  geom_boxplot() +
  labs(title = "CGPA by Teacher Consultation", x = "Teacher Consultation (Yes/No)", y = "CGPA") +
  theme_minimal()


# ------------------ CO_CURRICULAR vs CGPA ------------------

# Boxplot of CGPA by co-curricular involvement
ggplot(data, aes(x = co_curricular, y = cgpa, fill = co_curricular)) +
  geom_boxplot() +
  labs(title = "CGPA by Co-Curricular Participation", x = "Co-Curricular Activities (Yes/No)", y = "CGPA") +
  theme_minimal()


```

```{r, echo=FALSE, warning=FALSE}
# ============================================================================
# Relationship Between Study Behavior and CGPA Group (CGPA Categorical on X-axis)
# ============================================================================
# This section visualizes how study behavior variables vary across CGPA quartile groups (Low–Excellent).
# CGPA is discretized into 4 ordered groups.
# We examine:
# - Boxplots for continuous study behavior (study_hours, study_freq, attendance)
# - Barplots for binary/categorical behaviors (teacher_consult, co_curricular, learning_mode)
# This allows us to explore how learning patterns differ between academic performance groups.


# Filter CGPA in desired range and create CGPA groups
filtered_data <- data %>%
  filter(cgpa >= 1.5, cgpa <= 4) %>%
  mutate(cgpa_group = ntile(cgpa, 4)) %>%
  mutate(cgpa_group = factor(cgpa_group,
                             levels = 1:4,
                             labels = c("Low", "Average", "Good", "Excellent"))) %>%
  filter(!is.na(cgpa_group))  # make sure no NA groups

# Define variable types
numeric_vars <- c("study_hours", "study_freq", "attendance")
binary_vars <- c("teacher_consult", "co_curricular")
cat_vars <- c("learning_mode")

# 1. Boxplots for numeric variables
plot_list_num <- lapply(numeric_vars, function(var) {
  ggplot(filtered_data, aes_string(x = "cgpa_group", y = var, fill = "cgpa_group")) +
    geom_boxplot() +
    labs(title = paste("Distribution of", var, "by CGPA Group"),
         x = "CGPA Group", y = var) +
    theme_minimal() +
    theme(legend.position = "none")
})

# 2. Barplots for binary/categorical variables (side-by-side layout)
plot_list_cat <- lapply(c(binary_vars, cat_vars), function(var) {
  ggplot(filtered_data, aes_string(x = var, fill = "cgpa_group")) +
    geom_bar(position = "dodge") +
    labs(title = paste("Distribution of", var, "by CGPA Group"),
         x = var, y = "Count") +
    theme_minimal()
})

# 3. Display plots

for (p in plot_list_num) {
  print(p)
}

for (p in plot_list_cat) {
  print(p)
}
```

## Insights and references from previous research:

Across the four studies centred on learning-behaviour features, a clear pattern emerges: **objective and validated engagement data—LMS click traces, cross-checked attendance, and structured study-hour logs—yield strong predictive accuracy (≈ 0.80–0.90 AUC/accuracy), whereas unverified or coarse self-reports add little signal.** Moreover, simple study-time counts become non-significant once study-quality or prior performance is considered, emphasising that *how* students study matters more than *how long* they study.
For this question, this implies that models should prioritise high-resolution, validated behavioural metrics (e.g., platform activity, verified attendance, quality-of-study indicators) rather than relying solely on self-reported hours.

-   **Tao et al. (2024)** – XGBoost accuracy ≈ 0.90 using LMS logs (*VisitedResources*, *RaisedHands*, *StudentAbsenceDays*); gender and nationality each \< 5 % importance once behaviour variables were included.\
-   **Balakrishna et al. (2024)** – ANN AUC ≈ 0.81 with weekly study-hours and attendance verified against institutional records; *study_hours* held the highest feature weight (0.34).\
-   **Yılmaz & Şekeroğlu (2019)** – Random-Forest F-measure ≈ 0.84 on a proctored 31-item study-habit questionnaire; study-hours and attendance dominated importance, while age and gender were negligible.\
-   **Plant et al. (2005)** – Hierarchical regression showed raw study-time became non-significant (β ≈ 0) once deliberate-practice quality and prior GPA were added, explaining only \~ 2 % variance on its own.

------------------------------------------------------------------------

-   **Tao H., Hong L. & Al-Qadi S. (2024) – *Educational Data Mining for Student Performance Prediction***

    1.  **Title / Authors / Year**\
        Educational Data Mining for Student Performance Prediction – Tao H., Hong L.
        & Al-Qadi S.

        (2024) 

    2.  **Paper link**\
        <https://journal.esrgroups.org/jes/article/view/3434>

    3.  **Data link**\
        <https://www.kaggle.com/datasets/aljarah/xAPI-Edu-Data>

    4.  **Sample & how collected**\
        480 college students (private HEI, Jordan); LMS click metrics (*VisitedResources*, *RaisedHands*, *StudentAbsenceDays*) auto-logged by Kalboard 360; basic demographics from enrolment forms; target *Class* = Pass/Fail.

    5.  **Research question**\
        Can LMS behaviour plus minimal demographics predict Pass/Fail status?

    6.  **Models & top result**\
        Logistic Regression, SVM, Random Forest, **XGBoost accuracy ≈ 0.90** (10-fold CV).

    7.  **Conclusions / feature insights**\
        *StudentAbsenceDays*, *VisitedResources* and *RaisedHands* were strongest; gender & nationality each explained \< 5 % once behaviour traces were included.

    8.  **Feature quote**\
        “A predictive analytics approach … makes use of data sources, consisting of demographics, educational history, and behavioral tendencies … variables like attendance and engagement to expect a student will do in next exams or guides*” :contentReference[oaicite:1]{index="1"}

**consultation:**

Tao et al. achieve \~90 % accuracy largely because their predictors are objective, high-resolution LMS logs (0–100 click counts) and the target is a binary Pass/Fail label that is clearly separated in those logs.
Our survey data rely on self-reported frequencies that collapse into a few unbalanced categories (e.g., 85–90 % “High” attendance) and we try to predict a near-continuous CGPA or overlapping A–D bands.
The combination of weaker signal, class imbalance and a harder target naturally suppresses model performance.

```{r, echo=FALSE, warning=FALSE}
# -------------------------------------------------
# 1.  Read datasets
# -------------------------------------------------
# Load xAPI dataset and prepare your in-memory dataset (ours)
# Ensure relevant learning behavior variables are available

xapi <- readr::read_csv("research_data/xAPI-Edu-Data.csv",
                        show_col_types = FALSE)

ours <- data   # your cleaned dataset, assumed in memory
# Includes: study_hours, study_freq, teacher_consult,
#           co_curricular, attendance (0–100), cgpa, cgpa_class

# -------------------------------------------------
# 2.  Targeted recoding
# -------------------------------------------------

## 2.1 Recode xAPI: standardize factor levels
# - Class: L/M/H
# - Absence days: Low = under 7, High = 7+

xapi <- dplyr::mutate(
  xapi,
  Class = factor(Class, levels = c("L","M","H")),
  StudentAbsenceDays = dplyr::case_when(
    StudentAbsenceDays == "Under-7"  ~ "Low",
    StudentAbsenceDays == "Above-7"  ~ "High",
    TRUE                             ~ NA_character_
  ),
  StudentAbsenceDays = factor(StudentAbsenceDays,
                              levels = c("Low","High"))
)

## 2.2 Recode ours: prepare categorical features for comparison
# - attendance_cat: threshold at 75%
# - teacher_consult, co_curricular: Yes/No (binary factors)

ours <- dplyr::mutate(
  ours,
  attendance_cat  = factor(dplyr::if_else(attendance < 75, "Low", "High"),
                           levels = c("Low","High")),
  teacher_consult = factor(dplyr::if_else(teacher_consult == "Yes", "Yes", "No"),
                           levels = c("No","Yes")),
  co_curricular   = factor(dplyr::if_else(co_curricular == "Yes", "Yes", "No"),
                           levels = c("No","Yes"))
)

# -------------------------------------------------
# 3.  Variable mapping: xAPI → ours
# -------------------------------------------------
# Align variable names across datasets for plotting

var_map <- c(
  VisITedResources   = "study_hours",     # numeric
  Discussion         = "co_curricular",   # Yes / No
  raisedhands        = "teacher_consult", # Yes / No
  StudentAbsenceDays = "attendance_cat"   # Low / High
)

# -------------------------------------------------
# 4.  Plotting functions
# -------------------------------------------------

library(ggplot2)
library(gridExtra)
library(dplyr)

## 4.1 Distribution plots
# - Histograms for numeric, bar plots for categorical
# - Side-by-side: xAPI vs ours

plot_dist <- function(xapi_col, our_col) {

  p1 <- ggplot(xapi, aes(.data[[xapi_col]])) +
          { if(is.numeric(xapi[[xapi_col]]))
                geom_histogram(bins = 30, fill = "steelblue")
            else geom_bar(fill = "steelblue") } +
          labs(title = paste("xAPI –", xapi_col),
               x = xapi_col, y = "count") +
          theme_minimal()

  p2 <- ggplot(ours, aes(.data[[our_col]])) +
          { if(is.numeric(ours[[our_col]]))
                geom_histogram(bins = 30, fill = "darkorange")
            else geom_bar(fill = "darkorange") } +
          labs(title = paste("Our Data –", our_col),
               x = our_col, y = "count") +
          theme_minimal()

  grid.arrange(p1, p2, ncol = 2)
}

## 4.2 Predictor–target relationship
# - xAPI: bar plot or boxplot by final Class
# - Ours: boxplot or bar plot by cgpa_class

plot_vs_grade <- function(xapi_col, our_col) {

  # xAPI plot
  p1 <- if(is.numeric(xapi[[xapi_col]])) {
           ggplot(xapi, aes(x = Class,
                            y = .data[[xapi_col]],
                            fill = Class)) +
             geom_boxplot() +
             labs(title = paste("xAPI:", xapi_col, "~ Class"),
                  x = "Class (L–M–H)", y = xapi_col) +
             theme_minimal() +
             scale_fill_brewer(palette = "Set1")
         } else {
           ggplot(xapi, aes(x = .data[[xapi_col]], fill = Class)) +
             geom_bar(position = "dodge") +
             labs(title = paste("xAPI:", xapi_col, "~ Class"),
                  x = xapi_col, y = "count", fill = "Class") +
             theme_minimal()
         }

  # Our data plot
  p2 <- if(is.numeric(ours[[our_col]])) {
           ggplot(ours, aes(x = cgpa_class,
                            y = .data[[our_col]],
                            fill = cgpa_class)) +
             geom_boxplot() +
             labs(title = paste("Ours:", our_col, "~ cgpa_class"),
                  x = "cgpa_class (A–D)", y = our_col) +
             theme_minimal() +
             scale_fill_brewer(palette = "Set2")
         } else {
           ggplot(ours, aes(x = .data[[our_col]], fill = cgpa_class)) +
             geom_bar(position = "dodge") +
             labs(title = paste("Ours:", our_col, "~ cgpa_class"),
                  x = our_col, y = "count", fill = "cgpa_class") +
             theme_minimal()
         }

  grid.arrange(p1, p2, ncol = 2)
}

# -------------------------------------------------
# 5.  Generate plots
# -------------------------------------------------
# For each mapped variable:
# - Plot distribution comparison
# - Plot relationship to final grade/class

for (xp in names(var_map)) {

  op <- var_map[[xp]]

  cat("\n### Distribution:", xp, "<->", op, "\n")
  print( plot_dist(xp, op) )

  cat("\n### Relation:", xp, "<->", op, "\n")
  print( plot_vs_grade(xp, op) )
}

```

-   **Balakrishna R. et al. (2024) – *Predicting Student Results Based on Study Hours Using Machine Learning***

    1.  **Title / Authors / Year**\
        Predicting Student Results Based on Study Hours Using Machine Learning – Balakrishna R., Hegde A., Rao P. & Suresh K.

        (2024) 

    2.  **Paper link**\
        <https://ijirt.org/publishedpaper/IJIRT166525_PAPER.pdf>

    3.  **Data link**\
        <https://www.kaggle.com/datasets/spscientist/students-performance-in-exams>

    4.  **Sample & how collected**\
        ≈ 2 500 undergraduates (engineering, science, management); 32-item Google-Forms study-habits survey; attendance and grades cross-checked with institutional records.

    5.  **Research question**\
        Can weekly study-hours, verified attendance and learning-related social-media use predict GPA?

    6.  **Models & top result**\
        Artificial Neural Network (ANN) vs. Linear Regression → **ANN accuracy ≈ 0.77; ROC AUC ≈ 0.81**.

    7.  **Conclusions / feature insights**\
        ANN feature importances: *study_hours* 0.34 ▸ *social_media_for_learning* 0.22 ▸ *attendance* 0.14; study-time shows clear positive relation to GPA.

    8.  **Feature quote**\
        “*This study focuses in particular on how students’ use of social media and internet usage as learning resources affect their academic achievement.*” :contentReference[oaicite:2]{index="2"} "The findings show a substantial relationship between study time and academic achievement, with predicted and actual grades nearly matching."

**consultation:**

Balakrishna et al. obtain an AUC of ≈ 0.81 because attendance and weekly study-time were cross-checked against official records and students received a small grade bonus for accurate reporting.
This combination of external validation and positive incentive produced balanced, trustworthy features that correlate with GPA.
In our survey, self-reported hours and attendance are unverified, heavily skewed toward “always present/many hours,” and thus carry little signal.
Given the self-reported data collection and the skewed feature distributions, it is likely that our models will continue to perform only modestly unless we add some level of validation or redesign the variables to capture stronger signal.

```{r, echo=FALSE, warning=FALSE}
# -------------------------------------------------
# 1.  Read and prepare both datasets
# -------------------------------------------------
# Load the Student Performance (SP) dataset and your dataset (`ours`)
# Apply variable cleaning and consistency transformations

sp <- read_csv("research_data/data6article.csv") %>%
  clean_names()  # e.g., studyTimeweekly → study_time_weekly

sp <- sp %>%
  mutate(attendance = 1 - pmin(absences, 30) / 30)  # scale to [0–1]

ours <- data %>%
  clean_names()  # ensure variable name compatibility

# -------------------------------------------------
# 2.  Normalize and align feature formats
# -------------------------------------------------
# Standardize study time to weekly hours and scale attendance

ours <- ours %>%
  mutate(study_hours_weekly = study_hours * 7)

sp <- sp %>%
  mutate(attendance_percent = attendance * 100)

# -------------------------------------------------
# 3.  Variable mapping across datasets
# -------------------------------------------------
# Create mappings: SP variable → Ours variable
# Each pair represents a conceptually aligned feature

compare_vars <- list(
  attendance = "attendance",
  study_time_weekly = "study_hours_weekly"
)

target_vars <- list(
  gpa = "cgpa"
)

# -------------------------------------------------
# 4.  Plotting functions
# -------------------------------------------------

## 4.1 Compare distributions (hist/bar), side-by-side
plot_compare_dist <- function(sp_col, our_col) {
  sp_data <- if (sp_col == "attendance") {
    sp %>% rename(value = attendance_percent)
  } else {
    sp %>% rename(value = !!sym(sp_col))
  }

  our_data <- ours %>% rename(value = !!sym(our_col))

  p1 <- ggplot(sp_data, aes(x = value)) +
    { if (is.numeric(sp_data$value)) geom_histogram(fill = "steelblue", bins = 30)
      else geom_bar(fill = "steelblue") } +
    labs(title = paste("SP:", sp_col), x = sp_col) +
    theme_minimal()

  p2 <- ggplot(our_data, aes(x = value)) +
    { if (is.numeric(our_data$value)) geom_histogram(fill = "darkorange", bins = 30)
      else geom_bar(fill = "darkorange") } +
    labs(title = paste("Ours:", our_col), x = our_col) +
    theme_minimal()

  # Align x-axis range if both variables are numeric
  if (is.numeric(sp_data$value) && is.numeric(our_data$value)) {
    rng <- range(c(sp_data$value, our_data$value), na.rm = TRUE)
    p1 <- p1 + xlim(rng)
    p2 <- p2 + xlim(rng)
  }

  grid.arrange(p1, p2, ncol = 2)
}

## 4.2 Compare relationship to target outcome (scatter or boxplot)
plot_compare_target <- function(sp_col, our_col, sp_target, our_target) {
  sp_data <- if (sp_col == "attendance") {
    sp %>% mutate(value = attendance * 100)
  } else {
    sp %>% rename(value = !!sym(sp_col))
  }

  our_data <- ours %>% rename(value = !!sym(our_col))

  cor_sp <- if (is.numeric(sp_data$value)) {
    round(cor(sp_data$value, sp_data[[sp_target]], use = "complete.obs"), 3)
  } else NA

  cor_our <- if (is.numeric(our_data$value)) {
    round(cor(our_data$value, our_data[[our_target]], use = "complete.obs"), 3)
  } else NA

  p1 <- if (is.numeric(sp_data$value)) {
    ggplot(sp_data, aes(x = value, y = !!sym(sp_target))) +
      geom_point(alpha = 0.3) +
      geom_smooth(method = "lm", se = FALSE, color = "black") +
      labs(title = paste("SP:", sp_col, "vs", sp_target, "| r =", cor_sp)) +
      theme_minimal()
  } else {
    ggplot(sp_data, aes(x = value, y = !!sym(sp_target), fill = value)) +
      geom_boxplot() +
      labs(title = paste("SP:", sp_col, "vs", sp_target)) +
      theme_minimal()
  }

  p2 <- if (is.numeric(our_data$value)) {
    ggplot(our_data, aes(x = value, y = !!sym(our_target))) +
      geom_point(alpha = 0.3, color = "darkorange") +
      geom_smooth(method = "lm", se = FALSE, color = "black") +
      labs(title = paste("Ours:", our_col, "vs", our_target, "| r =", cor_our)) +
      theme_minimal()
  } else {
    ggplot(our_data, aes(x = value, y = !!sym(our_target), fill = value)) +
      geom_boxplot() +
      labs(title = paste("Ours:", our_col, "vs", our_target)) +
      theme_minimal()
  }

  grid.arrange(p1, p2, ncol = 2)
}

# -------------------------------------------------
# 5.  Run all comparisons
# -------------------------------------------------
# For each variable pair:
#   (1) show distribution comparison
#   (2) show relationship to target outcome

for (i in seq_along(compare_vars)) {
  sp_col  <- names(compare_vars)[i]
  our_col <- compare_vars[[i]]
  cat("\n\n##", sp_col, "<->", our_col, "\n")
  print(plot_compare_dist(sp_col, our_col))

  sp_target  <- names(target_vars)[1]
  our_target <- target_vars[[1]]
  print(plot_compare_target(sp_col, our_col, sp_target, our_target))
}


sp_tutor_counts <- sp %>%
  mutate(tutoring_cat = ifelse(tutoring == 1, "In Tutoring", "No Tutoring"))

our_tutor_counts <- ours %>%
  mutate(teacher_consult_cat = ifelse(teacher_consult == "Yes", "In Tutoring", "No Tutoring"))

p1 <- ggplot(sp_tutor_counts, aes(x = tutoring_cat)) +
  geom_bar(fill = "steelblue") +
  labs(title = "SP: tutoring", x = "Tutoring") +
  theme_minimal()

p2 <- ggplot(our_tutor_counts, aes(x = teacher_consult)) +
  geom_bar(fill = "darkorange") +
  labs(title = "Ours: teacher_consult", x = "teacher_consult") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)


sp_gpa_group <- sp %>%
  mutate(tutoring = factor(tutoring, levels = c(0, 1), labels = c("No", "Yes")))

ours_grouped <- ours %>%
  mutate(teacher_consult = factor(teacher_consult, levels = c("No", "Yes")),
         cgpa_class = factor(cgpa_class, levels = c("D", "C", "B", "A"))) 

p1 <- ggplot(sp_gpa_group, aes(x = tutoring, y = gpa, fill = tutoring)) +
  geom_boxplot() +
  labs(title = "SP: GPA vs Tutoring", x = "Tutoring", y = "GPA") +
  theme_minimal()

p2 <- ggplot(ours_grouped, aes(x = teacher_consult, y = cgpa, fill = teacher_consult)) +
  geom_boxplot() +
  labs(title = "Ours: CGPA vs teacher_consult", x = "Teacher Consult", y = "CGPA") +
  theme_minimal()

gridExtra::grid.arrange(p1, p2, ncol = 2)
```


-   **Yılmaz D. & Şekeroğlu B. (2019) – *Student Performance Classification Using AI Techniques***

    1.  **Title / Authors / Year**\
        Student Performance Classification Using Artificial Intelligence Techniques – Yılmaz D.
        & Şekeroğlu B.

        (2019) 

    2.  **Paper link**\
        <https://doi.org/10.1007/978-3-030-35249-3_76>

    3.  **Data link**\
        <https://archive.ics.uci.edu/dataset/856/higher%2Beducation%2Bstudents%2Bperformance%2Bevaluation>

    4.  **Sample & how collected**\
        145 undergraduates (Northern Cyprus, 2017-2018); 31-item structured questionnaire on study-habits, attendance, learning style, etc.; grade labels (A–F) verified with faculty records.

    5.  **Research question**\
        Can final letter-grades be classified accurately from learning-behaviour variables alone?

    6.  **Models & top result**\
        k-NN, C4.5, Naïve Bayes, Random Forest → **Random Forest F-measure ≈ 0.84** (10-fold CV).

    7.  **Conclusions / feature insights**\
        Weekly study-hours and class-attendance dominated importance; transport mode, age and gender contributed negligibly once study-habit variables were present.

    8.  **Feature quote**\
        “*Some of the main indicators are students’ personal information, educational preferences and family properties.*” :contentReference[oaicite:0]{index="0"}

**consultation:** While Yılmaz et al. report an F-measure ≈ 0.84, our own models plateaued at much lower scores.
A plausibleexplanation is that their data were gathered in a single, proctored session with fixed-choice categories, yielding clean, balanced distributions of study_hours and attendance and a discrete letter-grade target.
In contrast, our online, free-entry questionnaire, although complete, produced highly skewed study-hour buckets, several extreme outliers in the composite attend × study metric, and a continuous (or imbalanced) CGPA target.
These distributional and measurement differences reduce the effective signal available to tree-based or linear learners, even after feature engineering and class balancing.
Thus, disparities in both data collection protocol (controlled vs. self-reported) and target definition (clear-cut classes vs. overlapping CGPA bands) likely contribute to the performance gap, though only a full replication of Yılmaz’s preprocessing and model settings could confirm the exact impact.

```{r, echo=FALSE, warning=FALSE}
# -------------------------------------------------------------------
#  1.  READ DATA
# -------------------------------------------------------------------
# Load Yilmaz dataset and prepare local dataset (`ours`) for comparison

yilmaz <- read_csv("research_data/HigherEducationStudentsPerformanceEvaluation.csv")

ours <- data                                 # in-memory dataset
ours <- ours %>%
  mutate(attend_x_study = attendance * study_hours)

# -------------------------------------------------------------------
#  2.  RECODE “OURS”
# -------------------------------------------------------------------
# - Convert daily study hours to weekly buckets (match Yilmaz encoding)
# - Create CGPA classes (A–D) based on standard cutoffs

ours <- ours %>%
  mutate(
    weekly_study_hours = study_hours * 7,
    study_hours = case_when(
      weekly_study_hours == 0                     ~ "None",
      weekly_study_hours <  5                     ~ "<5",
      between(weekly_study_hours,  6, 10)         ~ "6–10",
      between(weekly_study_hours, 11, 20)         ~ "11–20",
      weekly_study_hours > 20                     ~ ">20",
      TRUE                                        ~ NA_character_
    ),
    study_hours = factor(study_hours,
                         levels = c("None", "<5", "6–10", "11–20", ">20")),
    cgpa_class = cut(
      cgpa,
      breaks = c(-Inf, 2.5, 3.0, 3.5, 4.0),
      labels = c("D", "C", "B", "A"),
      right  = TRUE
    )
  )

# -------------------------------------------------------------------
#  3.  RECODE “YILMAZ”
# -------------------------------------------------------------------
# - Columns 17 = study hours (1–5), 22 = attendance (1 = always)
# - Recode to match our buckets and create interaction feature

yilmaz <- yilmaz %>%
  mutate(
    study_hours = case_when(
      `17` == 1 ~ "None",
      `17` == 2 ~ "<5",
      `17` == 3 ~ "6–10",
      `17` == 4 ~ "11–20",
      `17` == 5 ~ ">20",
      TRUE      ~ NA_character_
    ),
    attendance = case_when(
      `22` == 1 ~ "Always",
      `22` == 2 ~ "Sometimes",
      `22` == 3 ~ "Never",
      TRUE      ~ NA_character_
    ),
    attend_x_study = as.numeric(`17`) * as.numeric(`22`),
    study_hours = factor(study_hours,
                         levels = c("None", "<5", "6–10", "11–20", ">20"))
  )

# -------------------------------------------------------------------
#  4.  COMPARISON HELPERS
# -------------------------------------------------------------------
# plot_compare_dist:
#   – side-by-side bar plots for variable distribution (Yilmaz vs Ours)
# plot_compare_target:
#   – Yilmaz: barplot by grade
#   – Ours: boxplot (by cgpa or cgpa_class)

plot_compare_dist <- function(yil_col, our_col, label) {
  p1 <- ggplot(yilmaz, aes(x = .data[[yil_col]])) +
          geom_bar(fill = "steelblue") +
          labs(title = paste("Yilmaz –", label), x = label, y = "count") +
          theme_minimal()

  p2 <- ggplot(ours, aes(x = .data[[our_col]])) +
          geom_bar(fill = "darkorange") +
          labs(title = paste("Our Data –", label), x = label, y = "count") +
          theme_minimal()

  grid.arrange(p1, p2, ncol = 2)
}

plot_compare_target <- function(yil_col, our_col, ours_outcome = c("cgpa", "cgpa_class"),
                                label) {

  ours_outcome <- match.arg(ours_outcome)

  # Yilmaz: bar plot with grade breakdown
  p1 <- ggplot(yilmaz, aes(x = .data[[yil_col]], fill = factor(GRADE))) +
          geom_bar(position = "dodge") +
          labs(title = paste("Yilmaz:", label, "vs GRADE"),
               x = label, y = "count", fill = "GRADE") +
          theme_minimal()

  # Ours: boxplot based on selected outcome
  if (ours_outcome == "cgpa") {
      p2 <- ggplot(ours, aes(x = .data[[our_col]], y = cgpa)) +
              geom_boxplot(fill = "orange") +
              labs(title = paste("Ours:", label, "vs cgpa"),
                   x = label, y = "cgpa") +
              theme_minimal()
  } else {
      p2 <- ggplot(ours, aes(x = cgpa_class, y = .data[[our_col]], fill = cgpa_class)) +
              geom_boxplot() +
              labs(title = paste("Ours:", label, "vs cgpa_class"),
                   x = "cgpa_class", y = label) +
              theme_minimal() +
              scale_fill_brewer(palette = "Set2")
  }

  grid.arrange(p1, p2, ncol = 2)
}

# -------------------------------------------------------------------
#  5.  PLOTS
# -------------------------------------------------------------------
# (a) distribution comparison: study_hours, attendance, interaction term
# (b) study_hours → cgpa (ours)
# (c) attendance → cgpa_class (ours)
# (d) attend_x_study → cgpa_class (ours)

compare_vars <- c("study_hours", "attendance", "attend_x_study")

for (var in compare_vars) {
  cat("\n### Distribution:", var, "\n")
  print(plot_compare_dist(var, var, var))
}

cat("\n### study_hours relationship\n")
print(
  plot_compare_target("study_hours", "study_hours",
                      ours_outcome = "cgpa", label = "study_hours")
)

cat("\n### attendance relationship\n")
print(
  plot_compare_target("attendance", "attendance",
                      ours_outcome = "cgpa_class", label = "attendance")
)

cat("\n### attend_x_study relationship\n")
print(
  plot_compare_target("attend_x_study", "attend_x_study",
                      ours_outcome = "cgpa_class", label = "attend_x_study")
)

```

-   **Plant E. A., Ericsson K. A., Hill L. & Åsberg K. (2005) – *Why Study Time Does Not Predict GPA Across College Students***

    1.  **Paper link**\
        <https://doi.org/10.1016/j.cedpsych.2004.06.001>

    2.  **Data link**\
        Not available (two-week study diaries + registrar GPA)

    3.  **Sample & method**\
        88 U.S. undergraduates kept daily logs of study hours, setting, and deliberate-practice behaviors; cumulative GPA pulled from university records.

    4.  **Research question**\
        Does raw study time still predict GPA after prior achievement and study-quality factors are considered?

    5.  **Models & key result**\
        Hierarchical regression → study time became *non-significant* (β≈0) once deliberate-practice quality and prior GPA were entered; raw hours explained \~2 % variance.

    6.  **Insight**\
        Quality-of-study (goal-directed, low-distraction, solitary practice) and verified attendance far outweigh mere quantity of hours.

    7.  **Representative quote**\
        "*the amount of study only emerged as a significant predictor of cumulative GPA when the quality of study and previously attained performance were taken into consideration.*"

**consultation:**

Across our survey and the four external datasets, the same pattern holds: raw study hours alone explain only \~1–2 % of the variance in GPA.
The external studies do better because they include validated attendance records and direct measures of study quality—e.g., whether a session had a clear goal, was done without distractions, or involved self-testing.
These quality-focused variables carry real predictive signal, whereas our dataset contains only self-reported “hours studied,” a quantity that says little about how effectively that time was used.

------------------------------------------------------------------------

## Model implementation and evaluation:

| Data set | Target | Best model | Test metric | Top-3 predictors (RF importance) |
|---------------|---------------|---------------|---------------|---------------|
| **Full** | `cgpa` | Linear Reg. | R² = 0.0065 · RMSE = 0.7825 | attendance, study_hours, study_freq |
| **Full** | `cgpa_scaled` | Linear Reg. | R² = 0.0065 · RMSE = 19.5633 | attendance, study_hours, study_freq |
| **Cleaned** | `cgpa` | Linear Reg. | R² = 0.0300 · RMSE = 0.4563 | attendance, study_hours, study_freq |
| **Cleaned** | `cgpa_scaled` | Linear Reg. | R² = 0.0300 · RMSE = 11.4074 | attendance, study_hours, study_freq |
| **Full** | `cgpa_class` | RF Classifier | Accuracy = 0.3167 · F1 = 0.3158 | attendance, study_hours, study_freq |
| **Cleaned** | `cgpa_class` | RF Classifier | Accuracy = 0.3246 · F1 = 0.3217 | attendance, study_hours, study_freq |

Modeling Summary – Learning Behavior Models

1.  **Predictive Power is Very Limited:**\
    Models based solely on learning behavior (study hours, frequency, attendance, etc.) exhibit **very low predictive performance**.
    Even the best regression models yield **R² ≤ 0.03** and classification accuracy stalls at **32%** — just slightly better than random guessing.
    Notably, misclassifications were not limited to adjacent CGPA groups; in several cases, predictions deviated by two or more levels (e.g., D↔A), indicating poor ordinal consistency and further limiting practical utility.

2.  **Minimal Gains from Cleaning:**\
    Removing outliers improves test RMSE by \~0.03 and adds up to 0.02 in R², but these changes are marginal.\
    Performance remains weak across all targets (`cgpa`, `cgpa_scaled`, `cgpa_class`) and datasets (full and cleaned).

3.  **Model Complexity Doesn’t Help:**\
    Linear Regression matches or outperforms Random Forest on regression tasks.\
    In classification, even Random Forest fails to extract meaningful signal from the features (precision, recall, and F1 ≈ 0.32).

4.  **Feature Signal is Concentrated and Weak:**\
    Across all models, `attendance` and `study_hours` consistently appear as the top predictors — but their **effect sizes are small**.\
    Variables like `learning_mode`, `teacher_consult`, and `co_curricular` show **near-zero importance** and statistically **insignificant** regression coefficients.

5.  **Conclusion:**\
    Learning behavior variables, even when combined, **fail to predict CGPA meaningfully**.\
    Results suggest that more informative inputs (e.g., prior grades, exam scores, online activity) are required to build effective prediction models.

```{r, echo=FALSE, warning=FALSE}
# -------------------------------------------------------------------
# Modeling Pipeline: Learning Behavior → GPA Prediction
# -------------------------------------------------------------------
# This script evaluates multiple models (linear regression, random forest)
# on learning-behavior predictors to forecast CGPA.
# Both regression and classification tasks are covered.

# Global container to collect all feature importances
all_importances <- list()

run_model_analysis <- function(data, target_col, task_type = c("regression", "classification"), dataset_name = "") {
  task_type <- match.arg(task_type)

  # Split data (80/20) with stratification by target
  set.seed(123)
  data_split <- initial_split(data, prop = 0.8, strata = !!sym(target_col))
  train_data <- training(data_split)
  test_data  <- testing(data_split)

  # Create 5-fold CV
  set.seed(123)
  cv_folds <- vfold_cv(train_data, v = 5, strata = !!sym(target_col))

  # Define predictors to include
  predictors <- c("study_freq", "study_hours", "co_curricular",
                   "teacher_consult", "learning_mode", "attendance")

  cat_vars <- c("learning_mode", "teacher_consult", "co_curricular")

  # Preprocessing recipe
  model_recipe <- recipe(as.formula(paste(target_col, "~", paste(predictors, collapse = "+"))), data = train_data) %>%
    step_mutate_at(cat_vars, fn = as.factor) %>%
    step_nzv(all_predictors()) %>%
    step_corr(all_numeric_predictors(), threshold = 0.9) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors())

  # Choose models
  if (task_type == "regression") {
    models <- list(
      "Linear Regression" = linear_reg() %>% set_engine("lm"),
      "Random Forest Regression" = rand_forest(mode = "regression", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  } else {
    models <- list(
      "Random Forest Classifier" = rand_forest(mode = "classification", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  }

  for (model_name in names(models)) {
    cat("\n Running:", model_name, "| Target:", target_col, "| Dataset:", dataset_name, "\n")

    wflow <- workflow() %>%
      add_model(models[[model_name]]) %>%
      add_recipe(model_recipe)

    # Manual CV loop (to allow custom metrics)
    cv_preds <- vector("list", length = 5)
    for (i in seq_along(cv_folds$splits)) {
      split <- cv_folds$splits[[i]]
      train_fold <- analysis(split)
      test_fold  <- assessment(split)
      model_fit  <- fit(wflow, data = train_fold)
      preds <- predict(model_fit, new_data = test_fold, type = ifelse(task_type == "classification", "class", "numeric")) %>%
        bind_cols(test_fold %>% select(all_of(target_col)))
      cv_preds[[i]] <- preds
    }

    all_cv_preds <- bind_rows(cv_preds)

    # Cross-validation metrics
    cat("\nCross-validation performance (5-fold):\n")
    if (task_type == "regression") {
      r1 <- rmse(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      cat(sprintf("RMSE: %.4f | R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, r3$.estimate))
    } else {
      a <- accuracy(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
    }

    # Test set performance
    final_fit <- fit(wflow, data = train_data)
    test_pred <- predict(final_fit, new_data = test_data, type = ifelse(task_type == "classification", "class", "numeric")) %>%
      bind_cols(test_data %>% select(all_of(target_col)))

    cat("\n Test set performance:\n")
    if (task_type == "regression") {
      r1 <- rmse(test_pred, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(test_pred, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(test_pred, truth = !!sym(target_col), estimate = .pred)
      adj_r2 <- 1 - (1 - r2$.estimate) * ((nrow(test_data) - 1) / (nrow(test_data) - length(predictors) - 1))
      cat(sprintf("RMSE: %.4f | R²: %.4f | Adjusted R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, adj_r2, r3$.estimate))
    } else {
      a <- accuracy(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(test_pred, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
      cat("\nConfusion Matrix:\n")
      print(conf_mat(test_pred, truth = !!sym(target_col), estimate = .pred_class))
    }

    # Variable importance or model coefficients
    model_fit <- extract_fit_parsnip(final_fit)
    if (inherits(model_fit$fit, "lm") || inherits(model_fit$fit, "glm")) {
      coef_df <- broom::tidy(model_fit$fit) %>%
        mutate(model = model_name, target = target_col, dataset = dataset_name)
      print(coef_df)
      all_importances[[length(all_importances) + 1]] <- coef_df
    } else if (!is.null(model_fit$fit$variable.importance)) {
      imp_tbl <- enframe(model_fit$fit$variable.importance, name = "feature", value = "importance") %>%
        arrange(desc(importance)) %>%
        mutate(model = model_name, target = target_col, dataset = dataset_name)
      print(imp_tbl)
      all_importances[[length(all_importances) + 1]] <- imp_tbl
    } else {
      cat("\nFeature importance not available for model:", model_name, "\n")
    }
  }
}

# -------------------------------------------------------------------
# Run pipeline across all data variants and targets
# -------------------------------------------------------------------

# Datasets: Full and Cleaned, Regression and Classification
all_data_variants <- list(
  "Full Data - Regr"    = data %>% select(-cgpa_class),
  "Cleaned Data - Regr" = data_clean %>% select(-cgpa_class),
  "Full Data - Class"   = data,
  "Cleaned Data - Class"= data_clean
)

# Target configuration list
targets <- list(
  list(col = "cgpa",         type = "regression",    dataset = "Full Data - Regr"),
  list(col = "cgpa_scaled",  type = "regression",    dataset = "Full Data - Regr"),
  list(col = "cgpa",         type = "regression",    dataset = "Cleaned Data - Regr"),
  list(col = "cgpa_scaled",  type = "regression",    dataset = "Cleaned Data - Regr"),
  list(col = "cgpa_class",   type = "classification",dataset = "Full Data - Class"),
  list(col = "cgpa_class",   type = "classification",dataset = "Cleaned Data - Class")
)

# Loop through all configurations and run analysis
for (target in targets) {
  dname <- target$dataset
  cat("\n=============================\n")
  cat("Dataset:", dname, "| Target:", target$col, "| Type:", target$type, "\n")
  cat("=============================\n")
  run_model_analysis(all_data_variants[[dname]], target$col, task_type = target$type, dataset_name = dname)
}
```

## Analyzing the question and running models across different subgroups in the dataset

Cross-Validated Model Performance by Sub-Group

*For sub-groups with fewer than 300 observations, only cross-validation (CV) metrics are reported; test-set scores were omitted intentionally.*

| Sub-group (*n*) | RF Accuracy (CV) | Best R² (CV) | Brief Interpretation |
|------------------|------------------|------------------|-------------------|
| **Income** (≈ 1 200) | 0.30 – 0.33 | ≤ 0.01 | Performance indistinguishable from the overall baseline across income brackets. |
| **Gender** (≈ 1 200) | 0.30 (Male) / 0.30 (Female) | **0.034** (Male) | Highest R² observed, yet still explains \< 4 % of CGPA variance. |
| **Age** (≈ 1 200) | 0.30 | 0.003 | Virtually no predictive advantage between younger and older students. |
| **English Level** (≈ 1 000) | 0.28 | ≈ 0.000 | Study-behaviour features provide almost no signal in this dimension. |
| **Interest = Software** (657) | 0.30 | 0.006 | Slight numerical lift; not practically meaningful. |
| **Skill = Programming** (813) | 0.27 | 0.017 | Minor signal; insufficient for actionable use. |

**Baseline (full sample):** Accuracy = 0.32 \| R² = 0.006

------------------------------------------------------------------------

Key Findings:

 1.
**Low predictive ceiling across segments:**\
Classification accuracy remains within 0.27 – 0.33, and R² never exceeds 0.04, regardless of the sub-group examined.

2.  **Gender shows the strongest - yet still weak signal:**\
    Even in the best case (male cohort), roughly 96 % of CGPA variance remains unexplained.

3.  **Very low-signal cohorts:**\
    Basic/intermediate English speakers and low-income students derive virtually no predictive benefit from study-behaviour variables.

4.  **Small sub-groups appropriately flagged:**\
    Metrics for *n* \< 300 groups were limited to CV to avoid over-interpreting unstable test scores.

```{r, echo=FALSE, warning=FALSE}
# Global list to collect feature importances
all_importances <- list()
# Global list to collect performance metrics
all_metrics <- list()

run_model_analysis <- function(data, target_col, task_type = c("regression", "classification"), dataset_name = "", group_name = "", full_analysis = TRUE) {
  task_type <- match.arg(task_type)

  set.seed(123)
  data_split <- initial_split(data, prop = 0.8, strata = !!sym(target_col))
  train_data <- training(data_split)
  test_data <- testing(data_split)

  set.seed(123)
  cv_folds <- vfold_cv(train_data, v = 5, strata = !!sym(target_col))

  predictors <- c("study_freq","study_hours", "co_curricular",
                  "teacher_consult", "learning_mode", "attendance", 
                  "Study_Attendance_Interaction", "Study_Efficiency", "TotalStudyEffort")

  cat_vars <- c( "learning_mode", "teacher_consult","co_curricular")
                

  model_recipe <- recipe(as.formula(paste(target_col, "~", paste(predictors, collapse = "+"))), data = train_data) %>%
    step_mutate_at(all_of(cat_vars), fn = as.factor) %>%
    step_nzv(all_predictors()) %>%
    step_corr(all_numeric_predictors(), threshold = 0.9) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors())

  if (task_type == "regression") {
    models <- list(
      "Linear Regression" = linear_reg() %>% set_engine("lm"),
      "Random Forest Regression" = rand_forest(mode = "regression", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  } else {
    models <- list(
      "Random Forest Classifier" = rand_forest(mode = "classification", trees = 500, mtry = 5, min_n = 10) %>%
        set_engine("ranger", importance = "permutation")
    )
  }

  for (model_name in names(models)) {
    cat("\nRunning:", model_name, "on target:", target_col, "with dataset:", dataset_name, "| Group:", group_name, "| n =", nrow(data), "\n")

    wflow <- workflow() %>%
      add_model(models[[model_name]]) %>%
      add_recipe(model_recipe)

    # Cross-validation metrics manually
    cv_preds <- vector("list", length = 5)
    for (i in seq_along(cv_folds$splits)) {
      split <- cv_folds$splits[[i]]
      train_fold <- analysis(split)
      test_fold <- assessment(split)
      model_fit <- fit(wflow, data = train_fold)
      preds <- predict(model_fit, new_data = test_fold, type = ifelse(task_type == "classification", "class", "numeric")) %>%
        bind_cols(test_fold %>% select(all_of(target_col)))
      cv_preds[[i]] <- preds
    }
    all_cv_preds <- bind_rows(cv_preds)

    cat("\nCross-validation performance (manual):\n")
    if (task_type == "regression") {
      r1 <- rmse(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r2 <- rsq(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      r3 <- mae(all_cv_preds, truth = !!sym(target_col), estimate = .pred)
      cat(sprintf("RMSE: %.4f | R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, r3$.estimate))
      
      # Store CV metrics
      cv_metrics <- data.frame(
        model = model_name,
        target = target_col,
        dataset = dataset_name,
        group = group_name,
        split = "CV",
        RMSE = r1$.estimate,
        R2 = r2$.estimate,
        MAE = r3$.estimate,
        stringsAsFactors = FALSE
      )
      all_metrics <<- append(all_metrics, list(cv_metrics))
      
    } else {
      a <- accuracy(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      r <- recall(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      p <- precision(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      f <- f_meas(all_cv_preds, truth = !!sym(target_col), estimate = .pred_class)
      cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
      
      # Store CV metrics
      cv_metrics <- data.frame(
        model = model_name,
        target = target_col,
        dataset = dataset_name,
        group = group_name,
        split = "CV",
        Accuracy = a$.estimate,
        Recall = r$.estimate,
        Precision = p$.estimate,
        F1 = f$.estimate,
        stringsAsFactors = FALSE
      )
      all_metrics <<- append(all_metrics, list(cv_metrics))
    }

    # Only run test analysis for groups with 300+ samples
    if (full_analysis) {
      # Final model
      final_fit <- fit(wflow, data = train_data)
      test_pred <- predict(final_fit, new_data = test_data, type = ifelse(task_type == "classification", "class", "numeric")) %>%
        bind_cols(test_data %>% select(all_of(target_col)))

      cat("\nTest set performance:\n")
      if (task_type == "regression") {
        r1 <- rmse(test_pred, truth = !!sym(target_col), estimate = .pred)
        r2 <- rsq(test_pred, truth = !!sym(target_col), estimate = .pred)
        r3 <- mae(test_pred, truth = !!sym(target_col), estimate = .pred)
        adj_r2 <- 1 - (1 - r2$.estimate) * ((nrow(test_data) - 1) / (nrow(test_data) - length(predictors) - 1))
        cat(sprintf("RMSE: %.4f | R²: %.4f | Adjusted R²: %.4f | MAE: %.4f\n", r1$.estimate, r2$.estimate, adj_r2, r3$.estimate))
        
        # Store test metrics
        test_metrics <- data.frame(
          model = model_name,
          target = target_col,
          dataset = dataset_name,
          group = group_name,
          split = "Test",
          RMSE = r1$.estimate,
          R2 = r2$.estimate,
          Adjusted_R2 = adj_r2,
          MAE = r3$.estimate,
          stringsAsFactors = FALSE
        )
        all_metrics <<- append(all_metrics, list(test_metrics))
        
      } else {
        a <- accuracy(test_pred, truth = !!sym(target_col), estimate = .pred_class)
        r <- recall(test_pred, truth = !!sym(target_col), estimate = .pred_class)
        p <- precision(test_pred, truth = !!sym(target_col), estimate = .pred_class)
        f <- f_meas(test_pred, truth = !!sym(target_col), estimate = .pred_class)
        cat(sprintf("Accuracy: %.4f | Recall: %.4f | Precision: %.4f | F1: %.4f\n", a$.estimate, r$.estimate, p$.estimate, f$.estimate))
        cat("\n Confusion Matrix:\n")
        print(conf_mat(test_pred, truth = !!sym(target_col), estimate = .pred_class))
        
        # Store test metrics
        test_metrics <- data.frame(
          model = model_name,
          target = target_col,
          dataset = dataset_name,
          group = group_name,
          split = "Test",
          Accuracy = a$.estimate,
          Recall = r$.estimate,
          Precision = p$.estimate,
          F1 = f$.estimate,
          stringsAsFactors = FALSE
        )
        all_metrics <<- append(all_metrics, list(test_metrics))
      }
      
      # Feature importance or coefficients - Store but don't print
      model_fit <- extract_fit_parsnip(final_fit)
      if (inherits(model_fit$fit, "lm") || inherits(model_fit$fit, "glm")) {
        coef_df <- broom::tidy(model_fit$fit) %>%
          mutate(model = model_name, target = target_col, dataset = dataset_name, group = group_name)
        all_importances[[length(all_importances) + 1]] <- coef_df
      } else if (!is.null(model_fit$fit$variable.importance)) {
        imp_tbl <- enframe(model_fit$fit$variable.importance, name = "feature", value = "importance") %>%
          arrange(desc(importance)) %>%
          mutate(model = model_name, target = target_col, dataset = dataset_name, group = group_name)
        all_importances[[length(all_importances) + 1]] <- imp_tbl
      }
    } else {
      cat("\nSmall sample size (n < 300) - Test metrics skipped to avoid overfitting\n")
    }
  }
}

# Dataset splitting to avoid cgpa_class leakage
all_data_variants <- list(
  "Full Data - Regr" = data %>% select(-cgpa_class),
  "Cleaned Data - Regr" = data_clean %>% select(-cgpa_class),
  "Full Data - Class" = data,
  "Cleaned Data - Class" = data_clean
)

targets <- list(
  list(col = "cgpa", type = "regression", dataset = "Full Data - Regr"),
  list(col = "cgpa_class", type = "classification", dataset = "Full Data - Class")
)

# Define grouping variables (removed the missing ones)
grouping_vars <- c("income", "gender", "age", "english_level", "interests_group", "skills_group")

# Function to create categorical groups from continuous variables
create_grouped_data <- function(data, group_var) {
  data_grouped <- data
  
  if (group_var == "income") {
    # First, let's see the distribution and create balanced groups
    income_quantiles <- quantile(data$income, probs = c(0, 0.33, 0.67, 1), na.rm = TRUE)
    data_grouped <- data_grouped %>%
      mutate(income_group = case_when(
        income <= income_quantiles[2] ~ paste0("Low Income (≤", round(income_quantiles[2]), ")"),
        income <= income_quantiles[3] ~ paste0("Medium Income (", round(income_quantiles[2]+1), "-", round(income_quantiles[3]), ")"), 
        income > income_quantiles[3] ~ paste0("High Income (>", round(income_quantiles[3]), ")"),
        TRUE ~ "Unknown"
      ))
    return(list(data = data_grouped, group_col = "income_group"))
    
  } else if (group_var == "age") {
    # Create balanced age groups based on data distribution
    age_quantiles <- quantile(data$age, probs = c(0, 0.5, 1), na.rm = TRUE)
    data_grouped <- data_grouped %>%
      mutate(age_group = case_when(
        age <= age_quantiles[2] ~ paste0("Younger (≤", round(age_quantiles[2]), ")"),
        age > age_quantiles[2] ~ paste0("Older (>", round(age_quantiles[2]), ")"),
        TRUE ~ "Unknown"
      ))
    return(list(data = data_grouped, group_col = "age_group"))
    
  } else if (group_var == "english_level") {
    # Add explanation for english levels (NA = Advanced)
    data_grouped <- data_grouped %>%
      mutate(english_level_labeled = case_when(
        english_level == 1 ~ "English Level 1 (Basic)",
        english_level == 2 ~ "English Level 2 (Intermediate)",
        english_level == 3 | is.na(english_level) ~ "English Level 3 (Advanced)",
        TRUE ~ paste0("English Level ", english_level)
      ))
    return(list(data = data_grouped, group_col = "english_level_labeled"))
    
  } else if (group_var == "interests_group") {
    # New grouping for interests
    data_grouped <- data_grouped %>%
      mutate(interests_combined = case_when(
        interests_group == "Data/AI" ~ "Data/AI",
        interests_group == "Hardware" ~ "Hardware",
        interests_group == "Software" ~ "Software",
        interests_group %in% c("Design", "Infrastructure", "Cyber", "Other") | is.na(interests_group) ~ "Other",
        TRUE ~ "Other"
      ))
    return(list(data = data_grouped, group_col = "interests_combined"))
    
  } else if (group_var == "skills_group") {
    # New grouping for skills
    data_grouped <- data_grouped %>%
      mutate(skills_combined = case_when(
        skills_group == "Programming" ~ "Programming",
        skills_group == "Infrastructure" ~ "Infrastructure",
        skills_group %in% c("Cyber", "Data_and_AI", "Design", "Other") | is.na(skills_group) ~ "Other",
        TRUE ~ "Other"
      ))
    return(list(data = data_grouped, group_col = "skills_combined"))
    
  } else {
    # For categorical variables, use as is
    return(list(data = data_grouped, group_col = group_var))
  }
}

# Function to run analysis for each group
run_group_analysis <- function(data_list, targets, group_var) {
  cat("\n\n=================================================================\n")
  cat("RUNNING ANALYSIS BY GROUP:", toupper(group_var), "\n")
  cat("=================================================================\n")
  
  for (target in targets) {
    dname <- target$dataset
    current_data <- data_list[[dname]]
    
    # Check if group variable exists in current dataset
    if (!group_var %in% names(current_data)) {
      cat("Warning: Group variable", group_var, "not found in dataset", dname, "\n")
      next
    }
    
    # Create grouped data
    grouped_result <- create_grouped_data(current_data, group_var)
    grouped_data <- grouped_result$data
    group_col <- grouped_result$group_col
    
    # Get unique values of the grouping variable
    group_values <- unique(grouped_data[[group_col]])
    group_values <- group_values[!is.na(group_values)]
    
    for (group_val in group_values) {
      cat("\n------------------------------------------------------------\n")
      cat("Dataset:", dname, "| Target:", target$col, "| Type:", target$type, "| Group:", group_var, "=", group_val, "\n")
      cat("------------------------------------------------------------\n")
      
      # Filter data for current group
      group_data <- grouped_data %>% filter(!!sym(group_col) == group_val)
      
      # Check if there's enough data
      if (nrow(group_data) < 20) {
        cat("Warning: Not enough data for group", group_var, "=", group_val, "(n =", nrow(group_data), "). Skipping.\n")
        next
      }
      
      # Check if we have enough data for full analysis
      full_analysis <- nrow(group_data) >= 300
      
      # Run analysis for this group
      tryCatch({
        run_model_analysis(group_data, target$col, task_type = target$type, 
                          dataset_name = dname, group_name = paste(group_var, group_val, sep = "="),
                          full_analysis = full_analysis)
      }, error = function(e) {
        cat("Error in analysis for group", group_var, "=", group_val, ":", e$message, "\n")
      })
    }
  }
}

# Create summary visualizations
create_summary_plots <- function() {
  
  # Load required libraries for plotting
  if (!require(ggplot2, quietly = TRUE)) {
    install.packages("ggplot2")
    library(ggplot2)
  }
  
  # Combine all metrics
  all_metrics_df <- bind_rows(all_metrics)
  
  if (nrow(all_metrics_df) == 0) {
    cat("No metrics data available for plotting.\n")
    return()
  }
  
  # Extract group variable from group column
  all_metrics_df <- all_metrics_df %>%
    mutate(
      group_var = case_when(
        str_detect(group, "^income") ~ "Income",
        str_detect(group, "^gender") ~ "Gender", 
        str_detect(group, "^age") ~ "Age",
        str_detect(group, "^skill=") ~ "Skill",
        str_detect(group, "^english_level") ~ "English Level",
        str_detect(group, "^interest_area") ~ "Interest Area",
        str_detect(group, "^interests_group") ~ "Interest Groups",
        str_detect(group, "^skills_group") ~ "Skill Groups",
        TRUE ~ "Other"
      ),
      group_value = str_extract(group, "(?<==).*")
    )
  
  # Filter for CV results only (instead of test)
  cv_metrics <- all_metrics_df %>%
    filter(split == "CV")
  
  # 1. Regression Performance (R²) across all groups
  regression_data <- cv_metrics %>%
    filter(target == "cgpa") %>%
    select(group_var, group_value, model, R2) %>%
    arrange(group_var, group_value)
    
  if (nrow(regression_data) > 0) {
    p1 <- ggplot(regression_data, aes(x = paste(group_var, group_value, sep = ": "), y = R2, fill = model)) +
      geom_col(position = "dodge", width = 0.7) +
      theme_minimal() +
      theme(
        axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5, size = 14),
        legend.position = "bottom"
      ) +
      labs(
        title = "Model Performance: R² Scores Across Groups (Cross-Validation)", 
        x = "Groups", 
        y = "R² Score (CV)",
        fill = "Model"
      ) +
      scale_fill_manual(values = c("Linear Regression" = "#1f77b4", "Random Forest Regression" = "#ff7f0e")) +
      ylim(0, 0.05)
    
    # Force the plot to display
    plot(p1)
  }
  
  # 2. Classification Performance (Accuracy) across all groups  
  classification_data <- cv_metrics %>%
    filter(target == "cgpa_class") %>%
    select(group_var, group_value, model, Accuracy) %>%
    arrange(group_var, group_value)
    
  if (nrow(classification_data) > 0) {
    p2 <- ggplot(classification_data, aes(x = paste(group_var, group_value, sep = ": "), y = Accuracy, fill = model)) +
      geom_col(position = "dodge", width = 0.7) +
      theme_minimal() +
      theme(
        axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5, size = 14),
        legend.position = "bottom"
      ) +
      labs(
        title = "Model Performance: Accuracy Scores Across Groups (Cross-Validation)", 
        x = "Groups", 
        y = "Accuracy (CV)",
        fill = "Model"
      ) +
      scale_fill_manual(values = c("Random Forest Classifier" = "#2ca02c")) +
      ylim(0, 0.6)
    
    # Force the plot to display
    plot(p2)
  }
  
  # 3. Summary comparison by group type
  summary_by_group_type <- cv_metrics %>%
    group_by(group_var, target, model) %>%
    summarise(
      avg_performance = case_when(
        target == "cgpa" ~ mean(R2, na.rm = TRUE),
        target == "cgpa_class" ~ mean(Accuracy, na.rm = TRUE)
      ),
      .groups = "drop"
    ) %>%
    mutate(
      metric_name = ifelse(target == "cgpa", "R² (CV)", "Accuracy (CV)")
    )
  
  if (nrow(summary_by_group_type) > 0) {
    p3 <- ggplot(summary_by_group_type, aes(x = group_var, y = avg_performance, fill = model)) +
      geom_col(position = "dodge", width = 0.7) +
      facet_wrap(~metric_name, scales = "free_y") +
      theme_minimal() +
      theme(
        axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5, size = 14),
        legend.position = "bottom"
      ) +
      labs(
        title = "Average Performance by Group Type", 
        x = "Group Type", 
        y = "Average Performance",
        fill = "Model"
      ) +
      scale_fill_brewer(type = "qual", palette = "Set2") +
      ylim(0, 0.4)
    
    # Force the plot to display
    plot(p3)
  }
  
  cat("\n SUMMARY COMPLETE!\n")
  cat("Generated plots showing model performance across all groups.\n")
}

# Run analysis for each grouping variable
for (group_var in grouping_vars) {
  run_group_analysis(all_data_variants, targets, group_var)
}

# Generate summary plots
create_summary_plots()
```

## Results analysis and conclusions

Executive Summary:

All analytical stages—descriptives, correlation checks, literature review and modelling—lead to the same conclusion: **self-reported learning-behaviour variables (attendance, study hours, study frequency, etc.) carry very limited predictive signal for CGPA**.
Feature distributions are highly skewed, inter-correlations are weak, and even after feature engineering the best models explain **≤ 3 %** of variance or achieve **≈ 32 %** classification accuracy, only marginally above chance.
Sub-group modelling (income, gender, age, English level, interests, skills) reveals no segment with materially better performance; the strongest cohort (males) still leaves **\> 96 %** of CGPA variance unexplained.

**Data-quality perspective:** External studies that reached 0.80–0.90 accuracy used *objective* predictors (LMS click counts, verified attendance, prior grades) whereas our dataset relies on unvalidated self-reports.
This difference in measurement fidelity, and not modelling technique, may accounts for the performance gap.

Key Modelling Results:

| Task / Data                | Best Model    | Metric   | Value     |
|----------------------------|---------------|----------|-----------|
| CGPA (regression, cleaned) | Linear Reg.   | R²       | **0.030** |
| CGPA scaled (RMSE units)   | Linear Reg.   | RMSE     | **11.4**  |
| CGPA class (A–D)           | RF Classifier | Accuracy | **0.325** |

Main Messages:

1.  **Low predictive ceiling** – Across the full sample and every major sub-group, performance plateaus at R² ≈ 0.03 and Accuracy ≈ 0.32.

2.  **Feature redundancy** – Engineered indices (e.g., *TotalStudyEffort*) are highly collinear with raw study-hour variables and add no incremental signal.

3.  **Measurement bias & noise** – The wide CGPA range among students claiming “100 % attendance” (CGPA 2.0–4.0) illustrates self-report distortion; similar noise is evident in study-hour responses.

4.  **Literature convergence** – High-performing studies succeed when data are objectively logged and include study-quality or prior-achievement dimensions; self-reports alone are insufficient in most of the caeses.

Recommendations for Future Work:

-   **Verified and richer data collection** – Plan for the systematic acquisition of *validated* inputs (e.g., swipe-based attendance, LMS engagement logs, formative-quiz accuracy) that capture *learning effectiveness*, not just effort.
    Such measures can widen feature variance and reduce self-report bias.

-   **Feature re-engineering to widen variance** – Convert narrowly clustered variables into representations with broader, information-rich ranges: normalise study hours, bin attendance into deciles rather than binary flags, or derive efficiency ratios (correct answers per study hour) to create features with higher dispersion.

**Summary:** The model’s poor performance may stem from three main issues: (1) data quality and reliability, including how the information was collected; (2) limited variance in key features, which restricts the model’s ability to learn; and (3) study-habit variables that reflect quantity of effort but not learning quality.
The relative impact of each factor requires further investigation.
